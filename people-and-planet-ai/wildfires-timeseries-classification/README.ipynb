{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j1pdgkM2PA9a"
      },
      "outputs": [],
      "source": [
        "#@title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
        "\n",
        "# Licensed to the Apache Software Foundation (ASF) under one\n",
        "# or more contributor license agreements. See the NOTICE file\n",
        "# distributed with this work for additional information\n",
        "# regarding copyright ownership. The ASF licenses this file\n",
        "# to you under the Apache License, Version 2.0 (the\n",
        "# \"License\"); you may not use this file except in compliance\n",
        "# with the License. You may obtain a copy of the License at\n",
        "#\n",
        "#   http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing,\n",
        "# software distributed under the License is distributed on an\n",
        "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
        "# KIND, either express or implied. See the License for the\n",
        "# specific language governing permissions and limitations\n",
        "# under the License."
      ],
      "id": "j1pdgkM2PA9a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orng0uT9-8iT"
      },
      "source": [
        "# Wildfire prediction and spread with Tensorflow and Vertex AI\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GoogleCloudPlatform/python-docs-samples/blob/main/people-and-planet-ai/land-cover-classification/README.ipynb)\n",
        "\n",
        "In 2021, wildfires destroyed [7 million acres of wildland](https://www.ncei.noaa.gov/access/monitoring/monthly-report/fire/202113)--roughly the same area as the state of Massachusetts. These wildfires destroyed homes, towns, and people's livelihoods--to say nothing of the loss of life of woodland animals.\n",
        "\n",
        "For a wildfire to catch hold and spread, a set of conditions must exist in an environment. These conditions have been measured and recorded in multiple sources--sources that are available in Earth Engine. Imagine if you could build a ML model that can predict the likelihood and spread of wildfires!\n",
        "\n",
        "This is an interactive notebook that contains all of the code necessary to train a simple Machine Learning model for wildfire prediction and spread.\n",
        "\n",
        "This is a first step introductory example of how these satellite images can be used to detect changes on the Earth.\n",
        "\n",
        "+ ‚è≤Ô∏è Time estimate: TT hours\n",
        "+ üí∞ Cost estimate: Around \\$DD USD (free if you use \\$300 Cloud credits)\n",
        "\n",
        "üíö This is one of many machine learning how-to samples inspired from real climate solutions aired on the People and Planet AI üé• series."
      ],
      "id": "orng0uT9-8iT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQzGEx55Nu3P",
        "outputId": "dd6fd377-e0df-40b5-befb-549cd3084486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "CQzGEx55Nu3P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsWGZW_fUJjN"
      },
      "source": [
        "## üìí Using this interactive notebook\n",
        "\n",
        "Click the **run** icons ‚ñ∂Ô∏è of each section within this notebook.\n",
        "\n",
        "![Run cell](data/images/run-cell.png)\n",
        "\n",
        "> üí° Alternatively, you can run the currently selected cell with `Ctrl + Enter` (or `‚åò + Enter` in a Mac).\n",
        "\n",
        "This **notebook code lets you train and deploy an ML model** from end-to-end. When you run a code cell, the code runs in the notebook's runtime, so you're not making any changes to your personal computer.\n",
        "\n",
        "> ‚ö†Ô∏è **To avoid any errors**, wait for each section to finish in their order before clicking the next ‚Äúrun‚Äù icon.\n",
        "\n",
        "This sample must be connected to a **Google Cloud project**, but nothing else is needed other than your Google Cloud project.\n",
        "\n",
        "You can use an _existing project_. Alternatively, you can create a new Cloud project [with cloud credits for free.](https://cloud.google.com/free/docs/gcp-free-tier)"
      ],
      "id": "jsWGZW_fUJjN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7-JZgREZHQK"
      },
      "source": [
        "## üö¥‚Äç‚ôÄÔ∏è Steps summary\n",
        "\n",
        "This notebook is friendly for _beginner_, _intermediate_, and _advanced_ users of geospatial, data analytics and machine learning.\n",
        "**No prior experience is needed** to dive in.\n",
        "\n",
        "Here's a quick summary of what you‚Äôll go through:\n",
        "\n",
        "1. **üìö Understand the data**:\n",
        "  Go through what we want to achieve and explore the data we want to use as _inputs and outputs_ for our model.\n",
        "\n",
        "1. TODO\n",
        "\n",
        "1. (Optional) **üõ† Delete the project** to avoid ongoing costs.\n"
      ],
      "id": "G7-JZgREZHQK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgtZrFNdP3lv"
      },
      "source": [
        "## üé¨ Before you begin\n",
        "\n",
        "We first need to install all the requirements for this notebook."
      ],
      "id": "DgtZrFNdP3lv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hArSnUbubJ0W",
        "outputId": "94aacc12-8424-4b68-ddb2-52f7ec5a272e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "apache-beam[gcp]==2.41.0\n",
        "earthengine-api==0.1.324\n",
        "folium==0.12.1.post1\n",
        "plotly==5.10.0"
      ],
      "id": "hArSnUbubJ0W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WplVt9PfbCWh",
        "outputId": "46477b11-1767-4473-f644-02e47131e5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing constraints.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile constraints.txt\n",
        "cachetools==4.2.4 # apache-beam requires cachetools<5\n",
        "fastavro==1.5.4\n",
        "fasteners==0.17.3\n",
        "google-api-python-client==1.12.11 # earthengine-api requires google-api-python-client<2\n",
        "google-apitools==0.5.31 # apache-beam requires google-apitools<0.5.32\n",
        "google-auth-httplib2==0.1.0\n",
        "google-auth==1.35.0 # earthengine-api requires google-api-python-client<2\n",
        "google-cloud-bigquery-storage==2.13.2 # apache-beam requires google-cloud-bigquery-storage<2.14\n",
        "google-cloud-bigquery==2.34.4 # apache-beam requires google-cloud-bigquery<3\n",
        "google-cloud-bigtable==1.7.2 # apache-beam requires google-cloud-bigtable<2\n",
        "google-cloud-core==2.3.2\n",
        "google-cloud-datastore==1.15.5 # apache-beam requires google-cloud-datastore<2\n",
        "google-cloud-dlp==3.8.0\n",
        "google-cloud-language==1.3.2 # apache-beam requires google-cloud-language<2\n",
        "google-cloud-pubsub==2.13.5\n",
        "google-cloud-pubsublite==1.4.2\n",
        "google-cloud-storage==2.5.0\n",
        "google-cloud-spanner==1.19.3 # apache-beam requires google-cloud-spanner<2\n",
        "google-cloud-resource-manager==1.6.1\n",
        "google-cloud-recommendations-ai==0.7.1"
      ],
      "id": "WplVt9PfbCWh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHNPnSZybYA5",
        "outputId": "7bc925fd-08e7-4e1a-804c-632d179fc34e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m526.2/526.2 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m270.6/270.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m265.8/265.8 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m435.1/435.1 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.2/134.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for earthengine-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for httplib2shim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-firestore 2.7.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip --quiet install --upgrade pip\n",
        "!pip --quiet install -r requirements.txt -c constraints.txt google-cloud-aiplatform\n",
        "\n",
        "# Restart the runtime by ending the runtime's process\n",
        "exit()"
      ],
      "id": "GHNPnSZybYA5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq3k4qzz6c8Q"
      },
      "source": [
        "## ‚ö†Ô∏è Restart the runtime\n",
        "\n",
        "Colab already comes with many dependencies pre-loaded.\n",
        "In order to ensure everything runs as expected, we have to **restart the runtime**. This allows Colab to load the latest versions of the libraries.\n",
        "\n",
        "![\"Runtime\" > \"Restart runtime\"](data/images/restart-runtime.png)"
      ],
      "id": "Rq3k4qzz6c8Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfaQ4Os7QBNn"
      },
      "source": [
        "# ‚òÅÔ∏è My Google Cloud resources\n",
        "\n",
        "First, choose the Google Cloud _location_ where you want to run this sample.\n",
        "A good place to start is by choosing your [Google Cloud location](https://cloud.google.com/compute/docs/regions-zones).\n",
        "\n",
        "> ‚ö†Ô∏è Make sure you choose a location\n",
        "> available for all products: [Cloud Storage](https://cloud.google.com/storage/docs/locations),\n",
        "> [Vertex AI](https://cloud.google.com/vertex-ai/docs/general/locations),\n",
        "> [Dataflow](https://cloud.google.com/dataflow/docs/resources/locations), and\n",
        "> [Cloud Run](https://cloud.google.com/run/docs/locations).\n",
        "\n",
        "> üí° Prefer locations that are geographically closer to you with\n",
        "> [low carbon emissions](https://cloud.google.com/sustainability/region-carbon), highlighted with the\n",
        "> ![Leaf](https://cloud.google.com/sustainability/region-carbon/gleaf.svg) icon.\n",
        "\n",
        "Make sure you have followed these steps to configure your Google Cloud project:\n",
        "\n",
        "1. Enable the APIs: _Dataflow, Earth Engine, Vertex AI, and Cloud Run_\n",
        "\n",
        "  <button>\n",
        "\n",
        "  [Click here to enable the APIs](https://console.cloud.google.com/flows/enableapi?apiid=dataflow.googleapis.com,earthengine.googleapis.com,aiplatform.googleapis.com,run.googleapis.com)\n",
        "  </button>\n",
        "\n",
        "1. Create a Cloud Storage bucket in your desired _location_.\n",
        "\n",
        "  <button>\n",
        "\n",
        "  [Click here to create a new Cloud Storage bucket](https://console.cloud.google.com/storage/create-bucket)\n",
        "  </button>\n",
        "\n",
        "1. Register your\n",
        "  [Compute Engine default service account](https://console.cloud.google.com/iam-admin/iam)\n",
        "  on Earth Engine.\n",
        "\n",
        "  <button>\n",
        "\n",
        "  [Click here to register your service account on Earth Engine](https://signup.earthengine.google.com/#!/service_accounts)\n",
        "  </button>\n",
        "\n",
        "Once you have everything ready, you can go ahead and fill in your Google Cloud resources in the following code cell.\n",
        "Make sure you run it!"
      ],
      "id": "RfaQ4Os7QBNn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVz5zhvZ1mM3",
        "outputId": "eb5ad7db-61a2-41a9-f3c8-cce227c74800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Please fill in these values.\n",
        "project = \"video-erschmid\" #@param {type:\"string\"}\n",
        "bucket = \"erschmid-wildfires\" #@param {type:\"string\"}\n",
        "location = \"us-west1\" #@param {type:\"string\"}\n",
        "\n",
        "# Load values from environment variables if available.\n",
        "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
        "bucket = os.environ.get(\"CLOUD_STORAGE_BUCKET\", bucket)\n",
        "location = os.environ.get(\"CLOUD_LOCATION\", location)\n",
        "\n",
        "# Quick input validations.\n",
        "assert project, \"‚ö†Ô∏è Please provide a Google Cloud project ID\"\n",
        "assert bucket, \"‚ö†Ô∏è Please provide a Cloud Storage bucket name\"\n",
        "assert not bucket.startswith('gs://'), f\"‚ö†Ô∏è Please remove the gs:// prefix from the bucket name: {bucket}\"\n",
        "assert location, \"‚ö†Ô∏è Please provide a Google Cloud location\"\n",
        "\n",
        "# Configure gcloud.\n",
        "!gcloud config set project {project}"
      ],
      "id": "fVz5zhvZ1mM3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUX87ic-uNNI"
      },
      "source": [
        "Next, we have to authenticate Earth Engine and initialize it.\n",
        "Since we've already authenticated to this [Colab](https://www.youtube.com/watch?v=rNgswRZ2C1Y) and saved them as the [Google default credentials](https://google-auth.readthedocs.io/en/master/reference/google.auth.html#google.auth.default),\n",
        "we can reuse those credentials for Earth Engine.\n",
        "\n",
        "> üí° Since we're making **large amounts of automated requests to Earth Engine**, we want to use the\n",
        "[high-volume endpoint](https://developers.google.com/earth-engine/cloud/highvolume)."
      ],
      "id": "fUX87ic-uNNI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TV2ZvLH17If"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import google.auth\n",
        "\n",
        "credentials, _ = google.auth.default(\n",
        "    scopes=[\n",
        "        \"https://www.googleapis.com/auth/cloud-platform\",\n",
        "        \"https://www.googleapis.com/auth/earthengine\",\n",
        "    ]\n",
        ")\n",
        "ee.Initialize(\n",
        "    credentials,\n",
        "    project=project,\n",
        "    opt_url=\"https://earthengine-highvolume.googleapis.com\",\n",
        ")"
      ],
      "id": "3TV2ZvLH17If"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V2XkHeNNMb_"
      },
      "source": [
        "# üìö Understand the data\n",
        "\n",
        "Before we begin, let's start by looking at what we want to achieve and the datasets we chose."
      ],
      "id": "6V2XkHeNNMb_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkVANXLpZCnd"
      },
      "source": [
        "## üéØ **Goal**: Time series forecasting and image segmentation\n",
        "\n",
        "The goal of our model is to use satellite images to analyze the likelihood and potential spread of wildfires for a given geographical region. The output layer will combine a time series forecast (likelihood of fire to spread) and a classification (on fire or not on fire)."
      ],
      "id": "GkVANXLpZCnd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqGsEZBf6ASC"
      },
      "source": [
        "## üõ∞ Inputs: Satellite images\n",
        "\n",
        "To achieve our goal, we must combine multiple geographical datasets into a single dataset (or map in this case). Each input--also known as \"features\" or \"independent variables\"--will be stored as a single band within the resulting map. The following list shows the datasets used for this example:\n",
        "\n",
        "* **USGS/SRTMGL1_003**: NASA SRTM Digital Elevation 30m\n",
        "* **GRIDMET/DROUGHT**: CONUS Drought Indices\n",
        "* **ECMWF/ERA5/DAILY**: Daily Aggregates - Latest Climate Reanalysis Produced by ECMWF / Copernicus Climate Change Service\n",
        "* **IDAHO_EPSCOR/GRIDMET**: University of Idaho Gridded Surface Meteorological Dataset\n",
        "* **CIESIN/GPWv411/GPW_Population_Density**: Population Density (Gridded Population of the World Version 4.11)\n",
        "\n",
        "The following table shows the model input variables, the source dataset, and the symbols used for variable in our model.\n",
        "\n",
        "| Feature | Original Source | Variable name |\n",
        "| --------|:----------------|:--------------|\n",
        "| Elevation | `USGS/SRTMGL1_003` | `elevation` |\n",
        "| Palmer Drought Severity Index | `GRIDMET/DROUGHT` | `psdi` |\n",
        "| Avg air temperature at 2m height | `ECMWF/ERA5/DAILY` | `mean_2m_air_temperature` |\n",
        "| Total precipitation | `ECMWF/ERA5/DAILY` | `total_precipitation` |\n",
        "| 10m u-component of wind (daily avg) | `ECMWF/ERA5/DAILY` | `u_component_of_wind_10m` |\n",
        "| 10m v-component of wind (daily avg) | `ECMWF/ERA5/DAILY` | `v_component_of_wind_10m'` |\n",
        "|\n",
        "| Precipatation amount | `IDAHO_EPSCOR/GRIDMET` | `pr` |\n",
        "| Specific humidity | `IDAHO_EPSCOR/GRIDMET` | `sph` |\n",
        "| Wind direction | `IDAHO_EPSCOR/GRIDMET` | `th` |\n",
        "| Minimum temperature | `IDAHO_EPSCOR/GRIDMET` | `tmmn` |\n",
        "| Maximum temperature | `IDAHO_EPSCOR/GRIDMET` | `tmmx` |\n",
        "| Wind velocity at 10m | `IDAHO_EPSCOR/GRIDMET` | `vs` |\n",
        "| Energy release component | `IDAHO_EPSCOR/GRIDMET` | `erc` |\n",
        "| Population density (per square km) | `CIESIN/GPWv411/GPW_Population_Density` | `population_density` |\n",
        "\n",
        "\n"
      ],
      "id": "aqGsEZBf6ASC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNpQO_6UQDOt"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "INPUTS = {\n",
        "    'USGS/SRTMGL1_003': [\"elevation\"],\n",
        "    'GRIDMET/DROUGHT': [\"psdi\"],\n",
        "    'ECMWF/ERA5/DAILY': [\n",
        "         'mean_2m_air_temperature',\n",
        "         'total_precipitation',\n",
        "         'u_component_of_wind_10m',\n",
        "         'v_component_of_wind_10m'],\n",
        "    'IDAHO_EPSCOR/GRIDMET': [\n",
        "         'pr',\n",
        "         'sph',\n",
        "         'th',\n",
        "         'tmmn',\n",
        "         'tmmx',\n",
        "         'vs',\n",
        "         'erc'],\n",
        "    'CIESIN/GPWv411/GPW_Population_Density': ['population_density'],\n",
        "    'MODIS/006/MOD14A1': ['FireMask']\n",
        "}\n",
        "\n",
        "\n",
        "SCALE = 5000\n",
        "WINDOW = timedelta(days=1)\n",
        "\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE = datetime(2020, 1, 1)"
      ],
      "id": "JNpQO_6UQDOt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_j0UGCkZavj"
      },
      "source": [
        "üó∫ **Outputs**: Land cover map\n",
        "\n",
        "Finally, we need to give the model a set of labels to apply to each section of the map. These labels tell the training program (Tensorflow) what we want to infer from the previous data. In other words, this dataset represents the \"dependent variable\" that our model attempts to predict. For our model, we will use the \"Terra Thermal Anomalies & Fire Daily Global 1km (MODIS/006/MOD14A1)\" map from Earth Engine. We'll use the column name `FireMask` for these labels."
      ],
      "id": "s_j0UGCkZavj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_xR5-c0ZhlN"
      },
      "outputs": [],
      "source": [
        "LABELS = {\n",
        "    'MODIS/006/MOD14A1': ['FireMask'],\n",
        "}\n"
      ],
      "id": "a_xR5-c0ZhlN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl0hrMIsrSyL"
      },
      "source": [
        "# üóÇ Explore the datasets\n",
        "\n",
        "We need at least two datasets, a _training_ and a _validation_ dataset, to train our model.\n",
        "They both have contain _examples_ of _inputs_ (features) with their respective _outputs_ (labels), but are used for two very different purposes.\n",
        "\n",
        "The _training dataset_ is what the model uses to learn and adjust itself.\n",
        "It goes through this dataset multiple times, similar to a student studying for an exam.\n",
        "\n",
        "The _validation dataset_ is used like an _exam_.\n",
        "It's important that the validation dataset does **not** include examples found in the training dataset, or the validation will be [biased](https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias).\n",
        "After going through the training dataset, the model will test itself against the validation dataset, which should include data it has not seen (learned from) before.\n",
        "\n",
        "For this sample, we'll fetch data from Earth Engine and use that to create our training and validation datasets."
      ],
      "id": "bl0hrMIsrSyL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppiChnJTaC9a"
      },
      "source": [
        "---\n",
        "\n",
        "**Note**\n",
        "\n",
        "The magnitude of the data in these datasets is huge: 16 bands of information for a significant sampling of rectangular areas. Because of the magnitude of this data, it is impractical to run an Apache Beam pipeline locally to build the datasets.\n",
        "\n",
        "(Luckily, we can use Cloud Dataflow for this very reason)\n",
        "\n",
        "In the following cells, we'll examine some of the code used to compose the dataset. However, when it comes to actually building the datasets, we will dispatch our code as a job in Dataflow using a single, discrete Python file named `create_dataset.py`.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "id": "ppiChnJTaC9a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JysWJJzdL8q"
      },
      "source": [
        "## üóæ Get patches from Earth Engine\n",
        "\n",
        "TODO"
      ],
      "id": "1JysWJJzdL8q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWHxmQhfdR3f"
      },
      "outputs": [],
      "source": [
        "@retry.Retry(deadline=60 * 20)  # seconds\n",
        "def ee_fetch(url: str) -> bytes:\n",
        "    # If we get \"429: Too Many Requests\" errors, it's safe to retry the request.\n",
        "    # The Retry library only works with `google.api_core` exceptions.\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 429:\n",
        "        raise exceptions.TooManyRequests(response.text)\n",
        "\n",
        "    # Still raise any other exceptions to make sure we got valid data.\n",
        "    response.raise_for_status()\n",
        "    return response.content\n",
        "\n",
        "\n",
        "def get_image(\n",
        "    date: datetime, bands_schema: Dict[str, List[str]], window: timedelta\n",
        ") -> ee.Image:\n",
        "    ee_init()\n",
        "    # if elevation dataset is part of bands_schema, deal with it separately\n",
        "    if 'USGS/SRTMGL1_003' in bands_schema:\n",
        "      elevation = ee.Image('USGS/SRTMGL1_003').select(bands_schema['USGS/SRTMGL1_003'])\n",
        "      bands_schema.pop(\"USGS/SRTMGL1_003\")\n",
        "    else:\n",
        "      elevation = None\n",
        "\n",
        "    # if population dataset is part of bands_schema, deal with it separately\n",
        "    if 'CIESIN/GPWv411/GPW_Population_Density' in bands_schema:\n",
        "      population = [\n",
        "          ee.ImageCollection('CIESIN/GPWv411/GPW_Population_Density')\n",
        "        .filterDate(date.isoformat(), (date + window).isoformat())\n",
        "        .select(bands_schema['CIESIN/GPWv411/GPW_Population_Density'])\n",
        "        .median()\n",
        "      ]\n",
        "      bands_schema.pop(\"CIESIN/GPWv411/GPW_Population_Density\")\n",
        "    else:\n",
        "      population = None\n",
        "\n",
        "    images = [\n",
        "        ee.ImageCollection(collection)\n",
        "        .filterDate(date.isoformat(), (date + window).isoformat())\n",
        "        .select(bands)\n",
        "        .mosaic()\n",
        "        for collection, bands in bands_schema.items()\n",
        "    ]\n",
        "    # add elevation to list\n",
        "    if elevation:\n",
        "      images.append(elevation)\n",
        "    # add population to list\n",
        "    if population:\n",
        "      images.append(population)\n",
        "    return ee.Image(images)\n",
        "\n",
        "def get_input_image(date: datetime) -> ee.Image:\n",
        "    return get_image(date, INPUTS, WINDOW)\n",
        "\n",
        "\n",
        "def get_label_image(date: datetime) -> ee.Image:\n",
        "    return get_image(date, LABELS, WINDOW)"
      ],
      "id": "bWHxmQhfdR3f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q26S625Ya2jF"
      },
      "source": [
        "## üìå Sample training points\n",
        "\n",
        "TODO"
      ],
      "id": "q26S625Ya2jF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsdWRszJSdKY"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import numpy as np\n",
        "\n",
        "from typing import NamedTuple\n",
        "\n",
        "class Bounds(NamedTuple):\n",
        "    west: float\n",
        "    south: float\n",
        "    east: float\n",
        "    north: float\n",
        "\n",
        "def sample_points(\n",
        "    image: ee.Image, num_points: int, bounds: Bounds, scale: int\n",
        ") -> np.ndarray:\n",
        "    def get_coordinates(point: ee.Feature) -> ee.Feature:\n",
        "        coords = point.geometry().coordinates()\n",
        "        return ee.Feature(None, {\"lat\": coords.get(1), \"lon\": coords.get(0)})\n",
        "\n",
        "    points = image.int().stratifiedSample(\n",
        "        num_points,\n",
        "        region=ee.Geometry.Rectangle(bounds),\n",
        "        scale=scale,\n",
        "        geometries=True,\n",
        "    )\n",
        "    url = points.map(get_coordinates).getDownloadURL(\"CSV\", [\"lat\", \"lon\"])\n",
        "    return np.genfromtxt(io.BytesIO(ee_fetch(url)), delimiter=\",\", skip_header=1)\n"
      ],
      "id": "xsdWRszJSdKY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_vP_MAbwlhh"
      },
      "source": [
        "# üöà Create the datasets in Dataflow\n",
        "\n",
        "We have all the pieces now, so lets put all together into an\n",
        "[Apache Beam](https://beam.apache.org/) pipeline.\n",
        "Apache Beam allows us to create parallel processing pipelines.\n",
        "\n",
        "For this pipeline, we will create a new file, `create_dataset.py`, that will will bundle along into our job to run on Cloud Dataflow.\n",
        "\n",
        "> üí° For more information on how to use Apache Beam, refer to the\n",
        "> [Tour of Beam](https://beam.apache.org/get-started/tour-of-beam/)"
      ],
      "id": "s_vP_MAbwlhh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlUXYsz9Ryqz",
        "outputId": "c26355f4-60a6-4427-8e9c-53172051eb68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting create_dataset.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile create_dataset.py\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Iterable, List, Optional, NamedTuple, Tuple\n",
        "import io\n",
        "import logging\n",
        "import random\n",
        "import requests\n",
        "import uuid\n",
        "\n",
        "import ee\n",
        "from google.api_core import retry, exceptions\n",
        "import google.auth\n",
        "import numpy as np\n",
        "from numpy.lib.recfunctions import structured_to_unstructured\n",
        "\n",
        "INPUTS = {\n",
        "    'USGS/SRTMGL1_003': [\"elevation\"],\n",
        "    'GRIDMET/DROUGHT': [\"psdi\"],\n",
        "    'ECMWF/ERA5/DAILY': [\n",
        "         'mean_2m_air_temperature',\n",
        "         'total_precipitation',\n",
        "         'u_component_of_wind_10m',\n",
        "         'v_component_of_wind_10m'],\n",
        "    'IDAHO_EPSCOR/GRIDMET': [\n",
        "         'pr',\n",
        "         'sph',\n",
        "         'th',\n",
        "         'tmmn',\n",
        "         'tmmx',\n",
        "         'vs',\n",
        "         'erc'],\n",
        "    'CIESIN/GPWv411/GPW_Population_Density': ['population_density'],\n",
        "    'MODIS/006/MOD14A1': ['FireMask']\n",
        "}\n",
        "\n",
        "LABELS = {\n",
        "    'MODIS/006/MOD14A1': ['FireMask'],\n",
        "}\n",
        "\n",
        "SCALE = 5000\n",
        "WINDOW = timedelta(days=1)\n",
        "\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE = datetime(2020, 1, 1)\n",
        "\n",
        "\n",
        "class Bounds(NamedTuple):\n",
        "    west: float\n",
        "    south: float\n",
        "    east: float\n",
        "    north: float\n",
        "\n",
        "\n",
        "class Point(NamedTuple):\n",
        "    lat: float\n",
        "    lon: float\n",
        "\n",
        "\n",
        "class Example(NamedTuple):\n",
        "    inputs: np.ndarray\n",
        "    labels: np.ndarray\n",
        "\n",
        "\n",
        "def ee_init() -> None:\n",
        "    \"\"\"Authenticate and initialize Earth Engine with the default credentials.\"\"\"\n",
        "    # Use the Earth Engine High Volume endpoint.\n",
        "    #   https://developers.google.com/earth-engine/cloud/highvolume\n",
        "    credentials, project = google.auth.default()\n",
        "    ee.Initialize(\n",
        "        credentials,\n",
        "        project=project,\n",
        "        opt_url=\"https://earthengine-highvolume.googleapis.com\",\n",
        "    )\n",
        "\n",
        "@retry.Retry(deadline=60 * 20)  # seconds\n",
        "def ee_fetch(url: str) -> bytes:\n",
        "    # If we get \"429: Too Many Requests\" errors, it's safe to retry the request.\n",
        "    # The Retry library only works with `google.api_core` exceptions.\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 429:\n",
        "        raise exceptions.TooManyRequests(response.text)\n",
        "\n",
        "    # Still raise any other exceptions to make sure we got valid data.\n",
        "    response.raise_for_status()\n",
        "    return response.content\n",
        "\n",
        "\n",
        "def get_image(\n",
        "    date: datetime, bands_schema: Dict[str, List[str]], window: timedelta\n",
        ") -> ee.Image:\n",
        "    ee_init()\n",
        "    # if elevation dataset is part of bands_schema, deal with it separately\n",
        "    if 'USGS/SRTMGL1_003' in bands_schema:\n",
        "      elevation = ee.Image('USGS/SRTMGL1_003').select(bands_schema['USGS/SRTMGL1_003'])\n",
        "      bands_schema.pop(\"USGS/SRTMGL1_003\")\n",
        "    else:\n",
        "      elevation = None\n",
        "\n",
        "    # if population dataset is part of bands_schema, deal with it separately\n",
        "    if 'CIESIN/GPWv411/GPW_Population_Density' in bands_schema:\n",
        "      population = [\n",
        "          ee.ImageCollection('CIESIN/GPWv411/GPW_Population_Density')\n",
        "        .filterDate(date.isoformat(), (date + window).isoformat())\n",
        "        .select(bands_schema['CIESIN/GPWv411/GPW_Population_Density'])\n",
        "        .median()\n",
        "      ]\n",
        "      bands_schema.pop(\"CIESIN/GPWv411/GPW_Population_Density\")\n",
        "    else:\n",
        "      population = None\n",
        "\n",
        "    images = [\n",
        "        ee.ImageCollection(collection)\n",
        "        .filterDate(date.isoformat(), (date + window).isoformat())\n",
        "        .select(bands)\n",
        "        .mosaic()\n",
        "        for collection, bands in bands_schema.items()\n",
        "    ]\n",
        "    # add elevation to list\n",
        "    if elevation:\n",
        "      images.append(elevation)\n",
        "    # add population to list\n",
        "    if population:\n",
        "      images.append(population)\n",
        "    return ee.Image(images)\n",
        "\n",
        "def get_input_image(date: datetime) -> ee.Image:\n",
        "    return get_image(date, INPUTS, WINDOW)\n",
        "\n",
        "\n",
        "def get_label_image(date: datetime) -> ee.Image:\n",
        "    return get_image(date, LABELS, WINDOW)\n",
        "\n",
        "\n",
        "def sample_labels(\n",
        "    date: datetime, num_points: int, bounds: Bounds\n",
        ") -> Iterable[Tuple[datetime, Point]]:\n",
        "    image = get_label_image(date)\n",
        "    for lat, lon in sample_points(image, num_points, bounds, SCALE):\n",
        "        yield (date, Point(lat, lon))\n",
        "\n",
        "\n",
        "def sample_points(\n",
        "    image: ee.Image, num_points: int, bounds: Bounds, scale: int\n",
        ") -> np.ndarray:\n",
        "    def get_coordinates(point: ee.Feature) -> ee.Feature:\n",
        "        coords = point.geometry().coordinates()\n",
        "        return ee.Feature(None, {\"lat\": coords.get(1), \"lon\": coords.get(0)})\n",
        "\n",
        "    points = image.int().stratifiedSample(\n",
        "        num_points,\n",
        "        region=ee.Geometry.Rectangle(bounds),\n",
        "        scale=scale,\n",
        "        geometries=True,\n",
        "    )\n",
        "    url = points.map(get_coordinates).getDownloadURL(\"CSV\", [\"lat\", \"lon\"])\n",
        "    return np.genfromtxt(io.BytesIO(ee_fetch(url)), delimiter=\",\", skip_header=1)\n",
        "\n",
        "\n",
        "def get_input_sequence(\n",
        "    date: datetime, point: Point, patch_size: int, num_days: int\n",
        ") -> np.ndarray:\n",
        "    dates = [date + timedelta(days=d) for d in range(1 - num_days, 1)]\n",
        "    images = [get_input_image(d) for d in dates]\n",
        "    return get_patch_sequence(images, point, patch_size, SCALE)\n",
        "\n",
        "\n",
        "def get_label_sequence(\n",
        "    date: datetime, point: Point, patch_size: int, num_days: int\n",
        ") -> np.ndarray:\n",
        "    dates = [date + timedelta(days=d) for d in range(1, num_days + 1)]\n",
        "    images = [get_label_image(d) for d in dates]\n",
        "    return get_patch_sequence(images, point, patch_size, SCALE)\n",
        "\n",
        "\n",
        "def get_training_example(\n",
        "    date: datetime, point: Point, patch_size: int = 64, num_days: int = 2\n",
        ") -> Example:\n",
        "    ee_init()\n",
        "    return Example(\n",
        "        get_input_sequence(date, point, patch_size, num_days + 1),\n",
        "        get_label_sequence(date, point, patch_size, num_days),\n",
        "    )\n",
        "\n",
        "def try_get_training_example(\n",
        "    date: datetime, point: Point, patch_size: int = 64, num_hours: int = 2\n",
        ") -> Iterable[Example]:\n",
        "    try:\n",
        "        yield get_training_example(date, point, patch_size, num_hours)\n",
        "    except Exception as e:\n",
        "        logging.exception(e)\n",
        "\n",
        "def get_patch_sequence(\n",
        "    image_sequence: List[ee.Image], point: Point, patch_size: int, scale: int\n",
        ") -> np.ndarray:\n",
        "    def unpack(arr: np.ndarray, i: int) -> np.ndarray:\n",
        "        names = [x for x in arr.dtype.names if x.startswith(f\"{i}_\")]\n",
        "        return np.moveaxis(structured_to_unstructured(arr[names]), -1, 0)\n",
        "\n",
        "    point = ee.Geometry.Point([point.lon, point.lat])\n",
        "    image = ee.ImageCollection(image_sequence).toBands()\n",
        "    url = image.getDownloadURL(\n",
        "        {\n",
        "            \"region\": point.buffer(scale * patch_size / 2, 1).bounds(1),\n",
        "            \"dimensions\": [patch_size, patch_size],\n",
        "            \"format\": \"NPY\",\n",
        "        }\n",
        "    )\n",
        "    flat_seq = np.load(io.BytesIO(ee_fetch(url)), allow_pickle=True)\n",
        "    return np.stack([unpack(flat_seq, i) for i, _ in enumerate(image_sequence)], axis=1)\n",
        "\n",
        "def write_npz_file(example: Example, file_prefix: str) -> str:\n",
        "    from apache_beam.io.filesystems import FileSystems\n",
        "\n",
        "    filename = FileSystems.join(file_prefix, f\"{uuid.uuid4()}.npz\")\n",
        "    with FileSystems.create(filename) as f:\n",
        "        np.savez_compressed(f, inputs=example.inputs, labels=example.labels)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def run(\n",
        "    output_path: str,\n",
        "    num_dates: int,\n",
        "    num_points: int,\n",
        "    bounds: Bounds,\n",
        "    patch_size: int,\n",
        "    max_requests: int,\n",
        "    beam_args: Optional[List[str]] = None,\n",
        ") -> None:\n",
        "    import apache_beam as beam\n",
        "    from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "    random_dates = [\n",
        "        START_DATE + (END_DATE - START_DATE) * random.random() for _ in range(num_dates)\n",
        "    ]\n",
        "\n",
        "    beam_options = PipelineOptions(\n",
        "        beam_args,\n",
        "        save_main_session=True,\n",
        "        requirements_file=\"requirements.txt\",\n",
        "        max_num_workers=max_requests,\n",
        "    )\n",
        "    with beam.Pipeline(options=beam_options) as pipeline:\n",
        "        (\n",
        "            pipeline\n",
        "            | \"Random dates\" >> beam.Create(random_dates)\n",
        "            | \"Sample labels\" >> beam.FlatMap(sample_labels, num_points, bounds)\n",
        "            | \"Reshuffle\" >> beam.Reshuffle()\n",
        "            | \"Get example\" >> beam.FlatMapTuple(try_get_training_example, patch_size)\n",
        "            | \"Write NPZ files\" >> beam.Map(write_npz_file, output_path)\n",
        "            | \"Log files\" >> beam.Map(logging.info)\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--output-path\", required=True)\n",
        "    parser.add_argument(\"--num-dates\", type=int, default=20)\n",
        "    parser.add_argument(\"--num-points\", type=int, default=10)\n",
        "    parser.add_argument(\"--west\", type=float, default=-125.3)\n",
        "    parser.add_argument(\"--south\", type=float, default=27.4)\n",
        "    parser.add_argument(\"--east\", type=float, default=-66.5)\n",
        "    parser.add_argument(\"--north\", type=float, default=49.1)\n",
        "    parser.add_argument(\"--patch-size\", type=int, default=64)\n",
        "    parser.add_argument(\"--max-requests\", type=int, default=20)\n",
        "    args, beam_args = parser.parse_known_args()\n",
        "\n",
        "    run(\n",
        "        output_path=args.output_path,\n",
        "        num_dates=args.num_dates,\n",
        "        num_points=args.num_points,\n",
        "        bounds=Bounds(args.west, args.south, args.east, args.north),\n",
        "        patch_size=args.patch_size,\n",
        "        max_requests=args.max_requests,\n",
        "        beam_args=beam_args,\n",
        "    )"
      ],
      "id": "OlUXYsz9Ryqz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4YA7C-nR9N4"
      },
      "source": [
        "# TESTING\n"
      ],
      "id": "Z4YA7C-nR9N4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSRdxu60Wz3g",
        "outputId": "1393ecb1-914d-4507-94bc-93ec0f6e599f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project {project}\n",
        "import os\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project"
      ],
      "id": "hSRdxu60Wz3g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSMI_351YXrR",
        "outputId": "f85ff7a7-3750-4884-cec6-800ac84ab2ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
            "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', '/tmp/tmpemi2rb87/tmp_requirements.txt', '--exists-action', 'i', '--no-deps', '--implementation', 'cp', '--abi', 'cp38', '--platform', 'manylinux2014_x86_64']\n",
            "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmp1wqz8omn', 'apache-beam==2.41.0', '--no-deps', '--no-binary', ':all:']\n",
            "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
            "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmp1wqz8omn', 'apache-beam==2.41.0', '--no-deps', '--only-binary', ':all:', '--python-version', '38', '--implementation', 'cp', '--abi', 'cp38', '--platform', 'manylinux1_x86_64']\n",
            "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.41.0-cp38-cp38-manylinux1_x86_64.whl\n",
            "INFO:apache_beam.runners.portability.sdk_container_builder:Compressed source files for building sdk container at /tmp/tmptuu4rv0b/source.tgz\n",
            "INFO:apache_beam.runners.portability.sdk_container_builder:Starting GCS upload to gs://erschmid-wildfires/fire/temp/source-361fada7-4fad-4b8b-b7cf-6273287b17e5.tgz...\n",
            "INFO:apache_beam.runners.portability.sdk_container_builder:Completed GCS upload to gs://erschmid-wildfires/fire/temp/source-361fada7-4fad-4b8b-b7cf-6273287b17e5.tgz.\n",
            "INFO:apache_beam.runners.portability.sdk_container_builder:Building sdk container with Google Cloud Build, this may take a few minutes, you may check build log at https://console.cloud.google.com/cloud-build/builds/62d3365c-ef1d-4dda-b1b5-7f73931b9751?project=147301782967\n",
            "INFO:apache_beam.runners.portability.sdk_container_builder:Python SDK container pre-build finished in 355.53 seconds\n",
            "INFO:apache_beam.runners.portability.sdk_container_builder:Python SDK container built and pushed as gcr.io/video-erschmid/fire/beam_python_prebuilt_sdk:361fada7-4fad-4b8b-b7cf-6273287b17e5.\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.8_sdk:2.41.0\n",
            "INFO:root:Using provided Python SDK container image: gcr.io/video-erschmid/fire/beam_python_prebuilt_sdk:361fada7-4fad-4b8b-b7cf-6273287b17e5\n",
            "INFO:root:Python SDK container image set to \"gcr.io/video-erschmid/fire/beam_python_prebuilt_sdk:361fada7-4fad-4b8b-b7cf-6273287b17e5\" for Docker environment\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fbfca249e50> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fbfca24a670> ====================\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://erschmid-wildfires/fire/temp\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/temp/beamapp-root-0106231918-311780-aj8dw4bl.1673047158.312493/pickled_main_session...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/temp/beamapp-root-0106231918-311780-aj8dw4bl.1673047158.312493/pickled_main_session in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/temp/beamapp-root-0106231918-311780-aj8dw4bl.1673047158.312493/pipeline.pb...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/temp/beamapp-root-0106231918-311780-aj8dw4bl.1673047158.312493/pipeline.pb in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
            " clientRequestId: '20230106231918313611-3302'\n",
            " createTime: '2023-01-06T23:19:21.256039Z'\n",
            " currentStateTime: '1970-01-01T00:00:00Z'\n",
            " id: '2023-01-06_15_19_20-9669864446478950301'\n",
            " location: 'us-west1'\n",
            " name: 'beamapp-root-0106231918-311780-aj8dw4bl'\n",
            " projectId: 'video-erschmid'\n",
            " stageStates: []\n",
            " startTime: '2023-01-06T23:19:21.256039Z'\n",
            " steps: []\n",
            " tempFiles: []\n",
            " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2023-01-06_15_19_20-9669864446478950301]\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2023-01-06_15_19_20-9669864446478950301\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-west1/2023-01-06_15_19_20-9669864446478950301?project=video-erschmid\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2023-01-06_15_19_20-9669864446478950301 is in state JOB_STATE_PENDING\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:20.713Z: JOB_MESSAGE_BASIC: Dataflow Runner V2 auto-enabled. Use --experiments=disable_runner_v2 to opt out.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:21.682Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2023-01-06_15_19_20-9669864446478950301. The number of workers will be between 1 and 20.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:21.769Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2023-01-06_15_19_20-9669864446478950301.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:23.887Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-west1-a.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:24.684Z: JOB_MESSAGE_DETAILED: Expanding SplittableParDo operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:24.718Z: JOB_MESSAGE_DETAILED: Expanding CollectionToSingleton operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:24.791Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:24.825Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Reshuffle/ReshufflePerKey/GroupByKey: GroupByKey not followed by a combiner.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:24.862Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey: GroupByKey not followed by a combiner.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:24.907Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:24.945Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:24.988Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.019Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/FlatMap(<lambda at core.py:3481>) into Random dates/Impulse\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.057Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys into Random dates/FlatMap(<lambda at core.py:3481>)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.099Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps) into Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.134Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.163Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Write into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.187Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.224Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps) into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.253Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.296Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/Map(decode) into Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.329Z: JOB_MESSAGE_DETAILED: Fusing consumer Sample labels into Random dates/Map(decode)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.357Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/AddRandomKeys into Sample labels\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.392Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/Map(reify_timestamps) into Reshuffle/AddRandomKeys\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.432Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/GroupByKey/Reify into Reshuffle/ReshufflePerKey/Map(reify_timestamps)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.471Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/GroupByKey/Write into Reshuffle/ReshufflePerKey/GroupByKey/Reify\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.497Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow into Reshuffle/ReshufflePerKey/GroupByKey/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.531Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps) into Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.572Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/RemoveRandomKeys into Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.607Z: JOB_MESSAGE_DETAILED: Fusing consumer Get example into Reshuffle/RemoveRandomKeys\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.641Z: JOB_MESSAGE_DETAILED: Fusing consumer Write NPZ files into Get example\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.674Z: JOB_MESSAGE_DETAILED: Fusing consumer Log files into Write NPZ files\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.711Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.737Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.763Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.798Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.941Z: JOB_MESSAGE_DEBUG: Executing wait step start23\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:25.991Z: JOB_MESSAGE_BASIC: Executing operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:26.043Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:26.075Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-west1-a...\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:26.375Z: JOB_MESSAGE_BASIC: Finished operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:26.430Z: JOB_MESSAGE_DEBUG: Value \"Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Session\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:26.483Z: JOB_MESSAGE_BASIC: Executing operation Random dates/Impulse+Random dates/FlatMap(<lambda at core.py:3481>)+Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps)+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Write\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2023-01-06_15_19_20-9669864446478950301 is in state JOB_STATE_RUNNING\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:19:57.861Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:23:06.340Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:23:32.172Z: JOB_MESSAGE_DETAILED: All workers have finished the startup processes and began to receive work requests.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:23:34.428Z: JOB_MESSAGE_BASIC: Finished operation Random dates/Impulse+Random dates/FlatMap(<lambda at core.py:3481>)+Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps)+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Write\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:23:34.495Z: JOB_MESSAGE_BASIC: Executing operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:23:34.570Z: JOB_MESSAGE_BASIC: Finished operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:23:34.632Z: JOB_MESSAGE_BASIC: Executing operation Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:23:34.787Z: JOB_MESSAGE_BASIC: Finished operation Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:23:34.852Z: JOB_MESSAGE_DEBUG: Value \"Reshuffle/ReshufflePerKey/GroupByKey/Session\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:23:34.911Z: JOB_MESSAGE_BASIC: Executing operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Read+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)+Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys+Random dates/Map(decode)+Sample labels+Reshuffle/AddRandomKeys+Reshuffle/ReshufflePerKey/Map(reify_timestamps)+Reshuffle/ReshufflePerKey/GroupByKey/Reify+Reshuffle/ReshufflePerKey/GroupByKey/Write\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:25:12.991Z: JOB_MESSAGE_DETAILED: Autoscaling: Resizing worker pool from 1 to 3.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:25:18.160Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 3 based on the rate of progress in the currently running stage(s).\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:30:09.772Z: JOB_MESSAGE_BASIC: Finished operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Read+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)+Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys+Random dates/Map(decode)+Sample labels+Reshuffle/AddRandomKeys+Reshuffle/ReshufflePerKey/Map(reify_timestamps)+Reshuffle/ReshufflePerKey/GroupByKey/Reify+Reshuffle/ReshufflePerKey/GroupByKey/Write\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:30:09.848Z: JOB_MESSAGE_BASIC: Executing operation Reshuffle/ReshufflePerKey/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:30:09.949Z: JOB_MESSAGE_BASIC: Finished operation Reshuffle/ReshufflePerKey/GroupByKey/Close\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:30:10.010Z: JOB_MESSAGE_BASIC: Executing operation Reshuffle/ReshufflePerKey/GroupByKey/Read+Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow+Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)+Reshuffle/RemoveRandomKeys+Get example+Write NPZ files+Log files\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:31:03.927Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 6 based on the rate of progress in the currently running stage(s).\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:31:03.962Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool to 6, though goal was 20.  This could be a quota issue.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:31:08.772Z: JOB_MESSAGE_DETAILED: Autoscaling: Resizing worker pool from 3 to 20.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T23:31:13.983Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 20 based on the rate of progress in the currently running stage(s).\n"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "from datetime import datetime\n",
        "bounds = [-124, 24, -73, 49]\n",
        "date = datetime(2020, 1, 1)\n",
        "output_path = \"gs://{bucket}/fire/large_data\"\n",
        "num_dates = \"250\"\n",
        "num_points = \"100\"\n",
        "runner = \"DataflowRunner\"\n",
        "region = \"{location}\"\n",
        "temp_location = \"gs://{bucket}/fire/temp\"\n",
        "prebuild_sdk_container_engine = \"cloud_build\"\n",
        "docker_registry_push_url = \"gcr.io/{project}/fire\"\n",
        "\n",
        "!python create_dataset.py \\\n",
        "  --output-path \"gs://{bucket}/fire/large_data\" \\\n",
        "  --num-dates \"250\" \\\n",
        "  --num-points \"100\" \\\n",
        "  --runner \"DataflowRunner\" \\\n",
        "  --project \"{project}\" \\\n",
        "  --region \"{location}\" \\\n",
        "  --temp_location \"gs://{bucket}/fire/temp\" \\\n",
        "  --prebuild_sdk_container_engine \"cloud_build\" \\\n",
        "  --docker_registry_push_url \"gcr.io/{project}/fire\""
      ],
      "id": "vSMI_351YXrR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejZG4kzpRr2K"
      },
      "outputs": [],
      "source": [
        "run(output_path=output_path, num_dates=num_date, num_points=num_points, bounds=Bounds(-124, 24, -73, 49), patch_size=64, max_request=20, beam_args=beam_args)"
      ],
      "id": "ejZG4kzpRr2K"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-8.m90",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m90"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}