{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1pdgkM2PA9a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
        "\n",
        "# Licensed to the Apache Software Foundation (ASF) under one\n",
        "# or more contributor license agreements. See the NOTICE file\n",
        "# distributed with this work for additional information\n",
        "# regarding copyright ownership. The ASF licenses this file\n",
        "# to you under the Apache License, Version 2.0 (the\n",
        "# \"License\"); you may not use this file except in compliance\n",
        "# with the License. You may obtain a copy of the License at\n",
        "#\n",
        "#   http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing,\n",
        "# software distributed under the License is distributed on an\n",
        "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
        "# KIND, either express or implied. See the License for the\n",
        "# specific language governing permissions and limitations\n",
        "# under the License."
      ],
      "id": "j1pdgkM2PA9a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wildfire prediction and spread with Tensorflow and Vertex AI\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GoogleCloudPlatform/python-docs-samples/blob/main/people-and-planet-ai/land-cover-classification/README.ipynb)\n",
        "\n",
        "In 2021, wildfires destroyed [7 million acres of wildland](https://www.ncei.noaa.gov/access/monitoring/monthly-report/fire/202113)--roughly the same area as the state of Massachusetts. These wildfires destroyed homes, towns, and people's livelihoods--to say nothing of the loss of life of woodland animals.\n",
        "\n",
        "For a wildfire to catch hold and spread, a set of conditions must exist in an environment. These conditions have been measured and recorded in multiple sources--sources that are available in Earth Engine. Imagine if you could build a ML model that can predict the likelihood and spread of wildfires!\n",
        "\n",
        "This is an interactive notebook that contains all of the code necessary to train a simple Machine Learning model for wildfire prediction and spread.\n",
        "\n",
        "This is a first step introductory example of how these satellite images can be used to detect changes on the Earth.\n",
        "\n",
        "+ ‚è≤Ô∏è Time estimate: TT hours\n",
        "+ üí∞ Cost estimate: Around \\$DD USD (free if you use \\$300 Cloud credits)\n",
        "\n",
        "üíö This is one of many machine learning how-to samples inspired from real climate solutions aired on the People and Planet AI üé• series."
      ],
      "metadata": {
        "id": "orng0uT9-8iT"
      },
      "id": "orng0uT9-8iT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìí Using this interactive notebook\n",
        "\n",
        "Click the **run** icons ‚ñ∂Ô∏è of each section within this notebook.\n",
        "\n",
        "![Run cell](data/images/run-cell.png)\n",
        "\n",
        "> üí° Alternatively, you can run the currently selected cell with `Ctrl + Enter` (or `‚åò + Enter` in a Mac).\n",
        "\n",
        "This **notebook code lets you train and deploy an ML model** from end-to-end. When you run a code cell, the code runs in the notebook's runtime, so you're not making any changes to your personal computer.\n",
        "\n",
        "> ‚ö†Ô∏è **To avoid any errors**, wait for each section to finish in their order before clicking the next ‚Äúrun‚Äù icon.\n",
        "\n",
        "This sample must be connected to a **Google Cloud project**, but nothing else is needed other than your Google Cloud project.\n",
        "\n",
        "You can use an _existing project_. Alternatively, you can create a new Cloud project [with cloud credits for free.](https://cloud.google.com/free/docs/gcp-free-tier)"
      ],
      "metadata": {
        "id": "jsWGZW_fUJjN"
      },
      "id": "jsWGZW_fUJjN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö¥‚Äç‚ôÄÔ∏è Steps summary\n",
        "\n",
        "This notebook is friendly for _beginner_, _intermediate_, and _advanced_ users of geospatial, data analytics and machine learning.\n",
        "**No prior experience is needed** to dive in.\n",
        "\n",
        "Here's a quick summary of what you‚Äôll go through:\n",
        "\n",
        "1. **üìö Understand the data**:\n",
        "  Go through what we want to achieve and explore the data we want to use as _inputs and outputs_ for our model.\n",
        "\n",
        "1. TODO\n",
        "\n",
        "1. (Optional) **üõ† Delete the project** to avoid ongoing costs.\n"
      ],
      "metadata": {
        "id": "G7-JZgREZHQK"
      },
      "id": "G7-JZgREZHQK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üé¨ Before you begin\n",
        "\n",
        "We first need to install all the requirements for this notebook."
      ],
      "metadata": {
        "id": "DgtZrFNdP3lv"
      },
      "id": "DgtZrFNdP3lv"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "apache-beam[gcp]==2.41.0\n",
        "earthengine-api==0.1.324\n",
        "folium==0.12.1.post1\n",
        "plotly==5.10.0"
      ],
      "metadata": {
        "id": "hArSnUbubJ0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d8bb7a-dbf7-4216-e30b-804d438d1e51"
      },
      "id": "hArSnUbubJ0W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile constraints.txt\n",
        "cachetools==4.2.4 # apache-beam requires cachetools<5\n",
        "fastavro==1.5.4\n",
        "fasteners==0.17.3\n",
        "google-api-python-client==1.12.11 # earthengine-api requires google-api-python-client<2\n",
        "google-apitools==0.5.31 # apache-beam requires google-apitools<0.5.32\n",
        "google-auth-httplib2==0.1.0\n",
        "google-auth==1.35.0 # earthengine-api requires google-api-python-client<2\n",
        "google-cloud-bigquery-storage==2.13.2 # apache-beam requires google-cloud-bigquery-storage<2.14\n",
        "google-cloud-bigquery==2.34.4 # apache-beam requires google-cloud-bigquery<3\n",
        "google-cloud-bigtable==1.7.2 # apache-beam requires google-cloud-bigtable<2\n",
        "google-cloud-core==2.3.2\n",
        "google-cloud-datastore==1.15.5 # apache-beam requires google-cloud-datastore<2\n",
        "google-cloud-dlp==3.8.0\n",
        "google-cloud-language==1.3.2 # apache-beam requires google-cloud-language<2\n",
        "google-cloud-pubsub==2.13.5\n",
        "google-cloud-pubsublite==1.4.2\n",
        "google-cloud-storage==2.5.0\n",
        "google-cloud-spanner==1.19.3 # apache-beam requires google-cloud-spanner<2\n",
        "google-cloud-resource-manager==1.6.1\n",
        "google-cloud-recommendations-ai==0.7.1"
      ],
      "metadata": {
        "id": "WplVt9PfbCWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72274db2-8a4f-43bf-de30-d04388c9cbdd"
      },
      "id": "WplVt9PfbCWh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing constraints.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip --quiet install --upgrade pip\n",
        "!pip --quiet install -r requirements.txt -c constraints.txt google-cloud-aiplatform\n",
        "\n",
        "# Restart the runtime by ending the runtime's process\n",
        "exit()"
      ],
      "metadata": {
        "id": "GHNPnSZybYA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ef5821-14ae-4336-b5b7-8b81a1ca2117"
      },
      "id": "GHNPnSZybYA5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m526.2/526.2 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m270.6/270.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m265.8/265.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m435.1/435.1 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.2/134.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for earthengine-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for httplib2shim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-firestore 2.7.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö†Ô∏è Restart the runtime\n",
        "\n",
        "Colab already comes with many dependencies pre-loaded.\n",
        "In order to ensure everything runs as expected, we have to **restart the runtime**. This allows Colab to load the latest versions of the libraries.\n",
        "\n",
        "![\"Runtime\" > \"Restart runtime\"](data/images/restart-runtime.png)"
      ],
      "metadata": {
        "id": "Rq3k4qzz6c8Q"
      },
      "id": "Rq3k4qzz6c8Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚òÅÔ∏è My Google Cloud resources\n",
        "\n",
        "First, choose the Google Cloud _location_ where you want to run this sample.\n",
        "A good place to start is by choosing your [Google Cloud location](https://cloud.google.com/compute/docs/regions-zones).\n",
        "\n",
        "> ‚ö†Ô∏è Make sure you choose a location\n",
        "> available for all products: [Cloud Storage](https://cloud.google.com/storage/docs/locations),\n",
        "> [Vertex AI](https://cloud.google.com/vertex-ai/docs/general/locations),\n",
        "> [Dataflow](https://cloud.google.com/dataflow/docs/resources/locations), and\n",
        "> [Cloud Run](https://cloud.google.com/run/docs/locations).\n",
        "\n",
        "> üí° Prefer locations that are geographically closer to you with\n",
        "> [low carbon emissions](https://cloud.google.com/sustainability/region-carbon), highlighted with the\n",
        "> ![Leaf](https://cloud.google.com/sustainability/region-carbon/gleaf.svg) icon.\n",
        "\n",
        "Make sure you have followed these steps to configure your Google Cloud project:\n",
        "\n",
        "1. Enable the APIs: _Dataflow, Earth Engine, Vertex AI, and Cloud Run_\n",
        "\n",
        "  <button>\n",
        "\n",
        "  [Click here to enable the APIs](https://console.cloud.google.com/flows/enableapi?apiid=dataflow.googleapis.com,earthengine.googleapis.com,aiplatform.googleapis.com,run.googleapis.com)\n",
        "  </button>\n",
        "\n",
        "1. Create a Cloud Storage bucket in your desired _location_.\n",
        "\n",
        "  <button>\n",
        "\n",
        "  [Click here to create a new Cloud Storage bucket](https://console.cloud.google.com/storage/create-bucket)\n",
        "  </button>\n",
        "\n",
        "1. Register your\n",
        "  [Compute Engine default service account](https://console.cloud.google.com/iam-admin/iam)\n",
        "  on Earth Engine.\n",
        "\n",
        "  <button>\n",
        "\n",
        "  [Click here to register your service account on Earth Engine](https://signup.earthengine.google.com/#!/service_accounts)\n",
        "  </button>\n",
        "\n",
        "Once you have everything ready, you can go ahead and fill in your Google Cloud resources in the following code cell.\n",
        "Make sure you run it!"
      ],
      "metadata": {
        "id": "RfaQ4Os7QBNn"
      },
      "id": "RfaQ4Os7QBNn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVz5zhvZ1mM3",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452af6ea-27f3-48b5-9d2c-e67c034b8514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Please fill in these values.\n",
        "project = \"video-erschmid\" #@param {type:\"string\"}\n",
        "bucket = \"erschmid-wildfires\" #@param {type:\"string\"}\n",
        "location = \"us-west1\" #@param {type:\"string\"}\n",
        "\n",
        "# Load values from environment variables if available.\n",
        "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
        "bucket = os.environ.get(\"CLOUD_STORAGE_BUCKET\", bucket)\n",
        "location = os.environ.get(\"CLOUD_LOCATION\", location)\n",
        "\n",
        "# Quick input validations.\n",
        "assert project, \"‚ö†Ô∏è Please provide a Google Cloud project ID\"\n",
        "assert bucket, \"‚ö†Ô∏è Please provide a Cloud Storage bucket name\"\n",
        "assert not bucket.startswith('gs://'), f\"‚ö†Ô∏è Please remove the gs:// prefix from the bucket name: {bucket}\"\n",
        "assert location, \"‚ö†Ô∏è Please provide a Google Cloud location\"\n",
        "\n",
        "# Configure gcloud.\n",
        "!gcloud config set project {project}"
      ],
      "id": "fVz5zhvZ1mM3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we have to authenticate Earth Engine and initialize it.\n",
        "Since we've already authenticated to this [Colab](https://www.youtube.com/watch?v=rNgswRZ2C1Y) and saved them as the [Google default credentials](https://google-auth.readthedocs.io/en/master/reference/google.auth.html#google.auth.default),\n",
        "we can reuse those credentials for Earth Engine.\n",
        "\n",
        "> üí° Since we're making **large amounts of automated requests to Earth Engine**, we want to use the\n",
        "[high-volume endpoint](https://developers.google.com/earth-engine/cloud/highvolume)."
      ],
      "metadata": {
        "id": "fUX87ic-uNNI"
      },
      "id": "fUX87ic-uNNI"
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import google.auth\n",
        "\n",
        "credentials, _ = google.auth.default(\n",
        "    scopes=[\n",
        "        \"https://www.googleapis.com/auth/cloud-platform\",\n",
        "        \"https://www.googleapis.com/auth/earthengine\",\n",
        "    ]\n",
        ")\n",
        "ee.Initialize(\n",
        "    credentials,\n",
        "    project=project,\n",
        "    opt_url=\"https://earthengine-highvolume.googleapis.com\",\n",
        ")"
      ],
      "metadata": {
        "id": "3TV2ZvLH17If"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3TV2ZvLH17If"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö Understand the data\n",
        "\n",
        "Before we begin, let's start by looking at what we want to achieve and the datasets we chose."
      ],
      "metadata": {
        "id": "6V2XkHeNNMb_"
      },
      "id": "6V2XkHeNNMb_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ **Goal**: Time series forecasting and image segmentation\n",
        "\n",
        "The goal of our model is to use satellite images to analyze the likelihood and potential spread of wildfires for a given geographical region. The output layer will combine a time series forecast (likelihood of fire to spread) and a classification (on fire or not on fire)."
      ],
      "metadata": {
        "id": "GkVANXLpZCnd"
      },
      "id": "GkVANXLpZCnd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ∞ Inputs: Satellite images\n",
        "\n",
        "To achieve our goal, we must combine multiple geographical datasets into a single dataset (or map in this case). Each input--also known as \"features\" or \"independent variables\"--will be stored as a single band within the resulting map. The following list shows the datasets used for this example:\n",
        "\n",
        "* **USGS/SRTMGL1_003**: NASA SRTM Digital Elevation 30m\n",
        "* **GRIDMET/DROUGHT**: CONUS Drought Indices\n",
        "* **ECMWF/ERA5/DAILY**: Daily Aggregates - Latest Climate Reanalysis Produced by ECMWF / Copernicus Climate Change Service\n",
        "* **IDAHO_EPSCOR/GRIDMET**: University of Idaho Gridded Surface Meteorological Dataset\n",
        "* **CIESIN/GPWv411/GPW_Population_Density**: Population Density (Gridded Population of the World Version 4.11)\n",
        "\n",
        "The following table shows the model input variables, the source dataset, and the symbols used for variable in our model.\n",
        "\n",
        "| Feature | Original Source | Variable name |\n",
        "| --------|:----------------|:--------------|\n",
        "| Elevation | `USGS/SRTMGL1_003` | `elevation` |\n",
        "| Palmer Drought Severity Index | `GRIDMET/DROUGHT` | `psdi` |\n",
        "| Avg air temperature at 2m height | `ECMWF/ERA5/DAILY` | `mean_2m_air_temperature` |\n",
        "| Total precipitation | `ECMWF/ERA5/DAILY` | `total_precipitation` |\n",
        "| 10m u-component of wind (daily avg) | `ECMWF/ERA5/DAILY` | `u_component_of_wind_10m` |\n",
        "| 10m v-component of wind (daily avg) | `ECMWF/ERA5/DAILY` | `v_component_of_wind_10m'` |\n",
        "|\n",
        "| Precipatation amount | `IDAHO_EPSCOR/GRIDMET` | `pr` |\n",
        "| Specific humidity | `IDAHO_EPSCOR/GRIDMET` | `sph` |\n",
        "| Wind direction | `IDAHO_EPSCOR/GRIDMET` | `th` |\n",
        "| Minimum temperature | `IDAHO_EPSCOR/GRIDMET` | `tmmn` |\n",
        "| Maximum temperature | `IDAHO_EPSCOR/GRIDMET` | `tmmx` |\n",
        "| Wind velocity at 10m | `IDAHO_EPSCOR/GRIDMET` | `vs` |\n",
        "| Energy release component | `IDAHO_EPSCOR/GRIDMET` | `erc` |\n",
        "| Population density (per square km) | `CIESIN/GPWv411/GPW_Population_Density` | `population_density` |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aqGsEZBf6ASC"
      },
      "id": "aqGsEZBf6ASC"
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "INPUTS = {\n",
        "    'USGS/SRTMGL1_003': [\"elevation\"],\n",
        "    'GRIDMET/DROUGHT': [\"psdi\"],\n",
        "    'ECMWF/ERA5/DAILY': [\n",
        "         'mean_2m_air_temperature',\n",
        "         'total_precipitation',\n",
        "         'u_component_of_wind_10m',\n",
        "         'v_component_of_wind_10m'],\n",
        "    'IDAHO_EPSCOR/GRIDMET': [\n",
        "         'pr',\n",
        "         'sph',\n",
        "         'th',\n",
        "         'tmmn',\n",
        "         'tmmx',\n",
        "         'vs',\n",
        "         'erc'],\n",
        "    'CIESIN/GPWv411/GPW_Population_Density': ['population_density'],\n",
        "    'MODIS/006/MOD14A1': ['FireMask']\n",
        "}\n",
        "\n",
        "\n",
        "SCALE = 5000\n",
        "WINDOW = timedelta(days=1)\n",
        "\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE = datetime(2020, 1, 1)"
      ],
      "metadata": {
        "id": "JNpQO_6UQDOt"
      },
      "id": "JNpQO_6UQDOt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üó∫ **Outputs**: Land cover map\n",
        "\n",
        "Finally, we need to give the model a set of labels to apply to each section of the map. These labels tell the training program (Tensorflow) what we want to infer from the previous data. In other words, this dataset represents the \"dependent variable\" that our model attempts to predict. For our model, we will use the \"Terra Thermal Anomalies & Fire Daily Global 1km (MODIS/006/MOD14A1)\" map from Earth Engine. We'll use the column name `FireMask` for these labels."
      ],
      "metadata": {
        "id": "s_j0UGCkZavj"
      },
      "id": "s_j0UGCkZavj"
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = {\n",
        "    'MODIS/006/MOD14A1': ['FireMask'],\n",
        "}\n"
      ],
      "metadata": {
        "id": "a_xR5-c0ZhlN"
      },
      "id": "a_xR5-c0ZhlN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üóÇ Create the datasets\n",
        "\n",
        "We need at least two datasets, a _training_ and a _validation_ dataset, to train our model.\n",
        "They both have contain _examples_ of _inputs_ (features) with their respective _outputs_ (labels), but are used for two very different purposes.\n",
        "\n",
        "The _training dataset_ is what the model uses to learn and adjust itself.\n",
        "It goes through this dataset multiple times, similar to a student studying for an exam.\n",
        "\n",
        "The _validation dataset_ is used like an _exam_.\n",
        "It's important that the validation dataset does **not** include examples found in the training dataset, or the validation will be [biased](https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias).\n",
        "After going through the training dataset, the model will test itself against the validation dataset, which should include data it has not seen (learned from) before.\n",
        "\n",
        "For this sample, we'll fetch data from Earth Engine and use that to create our training and validation datasets."
      ],
      "metadata": {
        "id": "bl0hrMIsrSyL"
      },
      "id": "bl0hrMIsrSyL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Sample training points\n",
        "\n"
      ],
      "metadata": {
        "id": "q26S625Ya2jF"
      },
      "id": "q26S625Ya2jF"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import data_table"
      ],
      "metadata": {
        "id": "geHVHmgA_pB2"
      },
      "id": "geHVHmgA_pB2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Iterable, List, Optional, NamedTuple, Tuple\n",
        "import io\n",
        "import logging\n",
        "import random\n",
        "import requests\n",
        "import uuid\n",
        "\n",
        "import ee\n",
        "from google.api_core import retry, exceptions\n",
        "import google.auth\n",
        "import numpy as np\n",
        "from numpy.lib.recfunctions import structured_to_unstructured\n",
        "\n",
        "\n",
        "class Bounds(NamedTuple):\n",
        "    west: float\n",
        "    south: float\n",
        "    east: float\n",
        "    north: float\n",
        "\n",
        "\n",
        "class Point(NamedTuple):\n",
        "    lat: float\n",
        "    lon: float\n",
        "\n",
        "\n",
        "class Example(NamedTuple):\n",
        "    inputs: np.ndarray\n",
        "    labels: np.ndarray\n",
        "\n",
        "\n",
        "@retry.Retry(deadline=60 * 20)  # seconds\n",
        "def ee_fetch(url: str) -> bytes:\n",
        "    # If we get \"429: Too Many Requests\" errors, it's safe to retry the request.\n",
        "    # The Retry library only works with `google.api_core` exceptions.\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 429:\n",
        "        raise exceptions.TooManyRequests(response.text)\n",
        "\n",
        "    # Still raise any other exceptions to make sure we got valid data.\n",
        "    response.raise_for_status()\n",
        "    return response.content\n",
        "\n",
        "\n",
        "def get_image(\n",
        "    date: datetime, bands_schema: Dict[str, List[str]], window: timedelta\n",
        ") -> ee.Image:\n",
        "    # if elevation dataset is part of bands_schema, deal with it separately\n",
        "    if 'USGS/SRTMGL1_003' in bands_schema:\n",
        "      elevation = ee.Image('USGS/SRTMGL1_003').select(bands_schema['USGS/SRTMGL1_003'])\n",
        "      bands_schema.pop(\"USGS/SRTMGL1_003\")\n",
        "    else:\n",
        "      elevation = None\n",
        "\n",
        "    # if population dataset is part of bands_schema, deal with it separately\n",
        "    if 'CIESIN/GPWv411/GPW_Population_Density' in bands_schema:\n",
        "      population = [\n",
        "          ee.ImageCollection('CIESIN/GPWv411/GPW_Population_Density')\n",
        "        .filterDate(date.isoformat(), (date + window).isoformat())\n",
        "        .select(bands_schema['CIESIN/GPWv411/GPW_Population_Density'])\n",
        "        .median()\n",
        "      ]\n",
        "      bands_schema.pop(\"CIESIN/GPWv411/GPW_Population_Density\")\n",
        "    else:\n",
        "      population = None\n",
        "\n",
        "    images = [\n",
        "        ee.ImageCollection(collection)\n",
        "        .filterDate(date.isoformat(), (date + window).isoformat())\n",
        "        .select(bands)\n",
        "        .mosaic()\n",
        "        for collection, bands in bands_schema.items()\n",
        "    ]\n",
        "    # add elevation to list\n",
        "    if elevation:\n",
        "      images.append(elevation)\n",
        "    # add population to list\n",
        "    if population:\n",
        "      images.append(population)\n",
        "    return ee.Image(images)\n",
        "\n",
        "def get_input_image(date: datetime) -> ee.Image:\n",
        "    return get_image(date, INPUTS, WINDOW)\n",
        "\n",
        "\n",
        "def get_label_image(date: datetime) -> ee.Image:\n",
        "    return get_image(date, LABELS, WINDOW)\n",
        "\n",
        "\n",
        "def sample_labels(\n",
        "    date: datetime, num_points: int, bounds: Bounds\n",
        ") -> Iterable[Tuple[datetime, Point]]:\n",
        "    image = get_label_image(date)\n",
        "    for lat, lon in sample_points(image, num_points, bounds, SCALE):\n",
        "        yield (date, Point(lat, lon))\n",
        "\n",
        "\n",
        "def sample_points(\n",
        "    image: ee.Image, num_points: int, bounds: Bounds, scale: int\n",
        ") -> np.ndarray:\n",
        "    def get_coordinates(point: ee.Feature) -> ee.Feature:\n",
        "        coords = point.geometry().coordinates()\n",
        "        return ee.Feature(None, {\"lat\": coords.get(1), \"lon\": coords.get(0)})\n",
        "\n",
        "    points = image.int().stratifiedSample(\n",
        "        num_points,\n",
        "        region=ee.Geometry.Rectangle(bounds),\n",
        "        scale=scale,\n",
        "        geometries=True,\n",
        "    )\n",
        "    url = points.map(get_coordinates).getDownloadURL(\"CSV\", [\"lat\", \"lon\"])\n",
        "    return np.genfromtxt(io.BytesIO(ee_fetch(url)), delimiter=\",\", skip_header=1)\n",
        "\n",
        "\n",
        "def get_input_sequence(\n",
        "    date: datetime, point: Point, patch_size: int, num_days: int\n",
        ") -> np.ndarray:\n",
        "    dates = [date + timedelta(days=d) for d in range(1 - num_days, 1)]\n",
        "    images = [get_input_image(d) for d in dates]\n",
        "    return get_patch_sequence(images, point, patch_size, SCALE)\n",
        "\n",
        "\n",
        "def get_label_sequence(\n",
        "    date: datetime, point: Point, patch_size: int, num_days: int\n",
        ") -> np.ndarray:\n",
        "    dates = [date + timedelta(days=d) for d in range(1, num_days + 1)]\n",
        "    images = [get_label_image(d) for d in dates]\n",
        "    return get_patch_sequence(images, point, patch_size, SCALE)\n",
        "\n",
        "\n",
        "def get_training_example(\n",
        "    date: datetime, point: Point, patch_size: int = 64, num_days: int = 2\n",
        ") -> Example:\n",
        "    return Example(\n",
        "        get_input_sequence(date, point, patch_size, num_days + 1),\n",
        "        get_label_sequence(date, point, patch_size, num_days),\n",
        "    )\n",
        "\n",
        "def try_get_training_example(\n",
        "    date: datetime, point: Point, patch_size: int = 64, num_hours: int = 2\n",
        ") -> Iterable[Example]:\n",
        "    try:\n",
        "        yield get_training_example(date, point, patch_size, num_hours)\n",
        "    except Exception as e:\n",
        "        logging.exception(e)\n",
        "\n",
        "def get_patch_sequence(\n",
        "    image_sequence: List[ee.Image], point: Point, patch_size: int, scale: int\n",
        ") -> np.ndarray:\n",
        "    def unpack(arr: np.ndarray, i: int) -> np.ndarray:\n",
        "        names = [x for x in arr.dtype.names if x.startswith(f\"{i}_\")]\n",
        "        return np.moveaxis(structured_to_unstructured(arr[names]), -1, 0)\n",
        "\n",
        "    point = ee.Geometry.Point([point.lon, point.lat])\n",
        "    image = ee.ImageCollection(image_sequence).toBands()\n",
        "    url = image.getDownloadURL(\n",
        "        {\n",
        "            \"region\": point.buffer(scale * patch_size / 2, 1).bounds(1),\n",
        "            \"dimensions\": [patch_size, patch_size],\n",
        "            \"format\": \"NPY\",\n",
        "        }\n",
        "    )\n",
        "    flat_seq = np.load(io.BytesIO(ee_fetch(url)), allow_pickle=True)\n",
        "    return np.stack([unpack(flat_seq, i) for i, _ in enumerate(image_sequence)], axis=1)\n",
        "\n",
        "def write_npz_file(example: Example, file_prefix: str) -> str:\n",
        "    from apache_beam.io.filesystems import FileSystems\n",
        "\n",
        "    filename = FileSystems.join(file_prefix, f\"{uuid.uuid4()}.npz\")\n",
        "    with FileSystems.create(filename) as f:\n",
        "        np.savez_compressed(f, inputs=example.inputs, labels=example.labels)\n",
        "    return filename"
      ],
      "metadata": {
        "id": "IvrORTWLN_q_"
      },
      "id": "IvrORTWLN_q_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöà Create the datasets in Beam\n",
        "\n",
        "We have all the pieces now, so lets put all together into an\n",
        "[Apache Beam](https://beam.apache.org/) pipeline.\n",
        "Apache Beam allows us to create parallel processing pipelines.\n",
        "\n",
        "TODO: Explanation\n",
        "\n",
        "> üí° For more information on how to use Apache Beam, refer to the\n",
        "> [Tour of Beam](https://beam.apache.org/get-started/tour-of-beam/)"
      ],
      "metadata": {
        "id": "s_vP_MAbwlhh"
      },
      "id": "s_vP_MAbwlhh"
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "bounds = Bounds(-124, 24, -73, 49)\n",
        "date = datetime(2020, 1, 1)\n",
        "image=get_input_image(date)\n",
        "output_path = \"gs://{bucket}/fire/large_data\"\n",
        "num_dates = 250\n",
        "num_points = 100\n",
        "runner = \"DataflowRunner\"\n",
        "region = f\"{location}\"\n",
        "temp_location = f\"gs://{bucket}/fire/temp\"\n",
        "staging_location = f\"gs://{bucket}/fire/staging\"\n",
        "prebuild_sdk_container_engine = \"cloud_build\"\n",
        "docker_registry_push_url = \"gcr.io/{project}/fire\"\n",
        "max_requests = 20\n",
        "patch_size=64"
      ],
      "metadata": {
        "id": "vSMI_351YXrR"
      },
      "id": "vSMI_351YXrR",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "random_dates = [\n",
        "    START_DATE + (END_DATE - START_DATE) * random.random() for _ in range(num_dates)\n",
        "]\n",
        "\n",
        "beam_options = PipelineOptions(\n",
        "    save_main_session=True,\n",
        "    requirements_file=\"requirements.txt\",\n",
        "    max_num_workers=max_requests,\n",
        "    runner=runner,\n",
        "    project=project,\n",
        "    region=region,\n",
        "    temp_location=temp_location,\n",
        "    staging_location=staging_location,\n",
        ")\n",
        "with beam.Pipeline(options=beam_options) as pipeline:\n",
        "    (\n",
        "        pipeline\n",
        "        | \"Random dates\" >> beam.Create(random_dates)\n",
        "        | \"Sample labels\" >> beam.FlatMap(sample_labels, num_points, bounds)\n",
        "        | \"Reshuffle\" >> beam.Reshuffle()\n",
        "        | \"Get example\" >> beam.FlatMapTuple(try_get_training_example, patch_size)\n",
        "        | \"Write NPZ files\" >> beam.Map(write_npz_file, output_path)\n",
        "        | \"Log files\" >> beam.Map(logging.info)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5d3e0HvnbJkf",
        "outputId": "c093bb09-ed95-4544-c449-e78f89682810"
      },
      "id": "5d3e0HvnbJkf",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-43d477b1-80b3-4fc1-8d13-0ec1c5b6788a.json']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    div.alert {\n",
              "      white-space: pre-line;\n",
              "    }\n",
              "  </style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
              "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://erschmid-wildfires/fire/staging for cache location.</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', '/tmp/tmpb1keglzc/tmp_requirements.txt', '--exists-action', 'i', '--no-deps', '--implementation', 'cp', '--abi', 'cp38', '--platform', 'manylinux2014_x86_64']\n",
            "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpuc3migv7', 'apache-beam==2.41.0', '--no-deps', '--no-binary', ':all:']\n",
            "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
            "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpuc3migv7', 'apache-beam==2.41.0', '--no-deps', '--only-binary', ':all:', '--python-version', '38', '--implementation', 'cp', '--abi', 'cp38', '--platform', 'manylinux1_x86_64']\n",
            "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.41.0-cp38-cp38-manylinux1_x86_64.whl\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.8_sdk:2.41.0\n",
            "INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python38:2.41.0\n",
            "INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python38:2.41.0\" for Docker environment\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f5c80afff70> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f5c80b00790> ====================\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/requirements.txt...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/requirements.txt in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/pickled_main_session...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/pickled_main_session in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/dataflow_python_sdk.tar...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/dataflow_python_sdk.tar in 1 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/earthengine-api-0.1.324.tar.gz...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/earthengine-api-0.1.324.tar.gz in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/plotly-5.10.0-py2.py3-none-any.whl...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/plotly-5.10.0-py2.py3-none-any.whl in 18 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/apache_beam-2.41.0-cp38-cp38-manylinux1_x86_64.whl...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/apache_beam-2.41.0-cp38-cp38-manylinux1_x86_64.whl in 16 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/folium-0.12.1.post1-py2.py3-none-any.whl...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/folium-0.12.1.post1-py2.py3-none-any.whl in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/pipeline.pb...\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/staging/beamapp-root-0106191550-326731-zbtqihsc.1673032550.326888/pipeline.pb in 0 seconds.\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
            " clientRequestId: '20230106191550327805-7036'\n",
            " createTime: '2023-01-06T19:16:32.812775Z'\n",
            " currentStateTime: '1970-01-01T00:00:00Z'\n",
            " id: '2023-01-06_11_16_31-4561076491535887765'\n",
            " location: 'us-west1'\n",
            " name: 'beamapp-root-0106191550-326731-zbtqihsc'\n",
            " projectId: 'video-erschmid'\n",
            " stageStates: []\n",
            " startTime: '2023-01-06T19:16:32.812775Z'\n",
            " steps: []\n",
            " tempFiles: []\n",
            " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2023-01-06_11_16_31-4561076491535887765]\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2023-01-06_11_16_31-4561076491535887765\n",
            "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-west1/2023-01-06_11_16_31-4561076491535887765?project=video-erschmid\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2023-01-06_11_16_31-4561076491535887765 is in state JOB_STATE_PENDING\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:31.484Z: JOB_MESSAGE_BASIC: Dataflow Runner V2 auto-enabled. Use --experiments=disable_runner_v2 to opt out.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:33.218Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2023-01-06_11_16_31-4561076491535887765. The number of workers will be between 1 and 20.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:33.276Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2023-01-06_11_16_31-4561076491535887765.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:35.482Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-west1-a.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.124Z: JOB_MESSAGE_DETAILED: Expanding SplittableParDo operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.158Z: JOB_MESSAGE_DETAILED: Expanding CollectionToSingleton operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.231Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.266Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step [28]: Reshuffle/ReshufflePerKey/GroupByKey: GroupByKey not followed by a combiner.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.300Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey: GroupByKey not followed by a combiner.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.329Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.358Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.391Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.423Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Random dates/FlatMap(<lambda at core.py:3481>) into [28]: Random dates/Impulse\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.456Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys into [28]: Random dates/FlatMap(<lambda at core.py:3481>)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.485Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps) into [28]: Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.509Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify into [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.542Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Write into [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.576Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow into [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.601Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps) into [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.634Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys into [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.670Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Random dates/Map(decode) into [28]: Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.694Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Sample labels into [28]: Random dates/Map(decode)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.719Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Reshuffle/AddRandomKeys into [28]: Sample labels\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.753Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Reshuffle/ReshufflePerKey/Map(reify_timestamps) into [28]: Reshuffle/AddRandomKeys\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.788Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Reshuffle/ReshufflePerKey/GroupByKey/Reify into [28]: Reshuffle/ReshufflePerKey/Map(reify_timestamps)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.822Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Reshuffle/ReshufflePerKey/GroupByKey/Write into [28]: Reshuffle/ReshufflePerKey/GroupByKey/Reify\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.855Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow into [28]: Reshuffle/ReshufflePerKey/GroupByKey/Read\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.890Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps) into [28]: Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.924Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Reshuffle/RemoveRandomKeys into [28]: Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.948Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Get example into [28]: Reshuffle/RemoveRandomKeys\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:36.982Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Write NPZ files into [28]: Get example\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.018Z: JOB_MESSAGE_DETAILED: Fusing consumer [28]: Log files into [28]: Write NPZ files\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.057Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.086Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.119Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.144Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.292Z: JOB_MESSAGE_DEBUG: Executing wait step start23\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.361Z: JOB_MESSAGE_BASIC: Executing operation [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.411Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.442Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-west1-a...\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.750Z: JOB_MESSAGE_BASIC: Finished operation [28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.808Z: JOB_MESSAGE_DEBUG: Value \"[28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Session\" materialized.\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:16:37.890Z: JOB_MESSAGE_BASIC: Executing operation [28]: Random dates/Impulse+[28]: Random dates/FlatMap(<lambda at core.py:3481>)+[28]: Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys+[28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps)+[28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify+[28]: Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Write\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2023-01-06_11_16_31-4561076491535887765 is in state JOB_STATE_RUNNING\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:17:18.233Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
            "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-01-06T19:17:53.852Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-166f330a6069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam_options\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     (\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m|\u001b[0m \u001b[0;34m\"Random dates\"\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1656\u001b[0m       \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m       \u001b[0;31m# TODO: Merge the termination code in poll_for_job_completion and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-8.m90",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m90"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}