{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering with pandas and scikit-learn\n",
    "\n",
    "This notebook demonstrates how to use AI Platform notebooks to perform feature engineering on a dataset using `pandas`.\n",
    "\n",
    "Load the data into a `Pandas DataFrame`, clean up the columns into a usable format, and then restructure the data into feature and target data columns. \n",
    "\n",
    "Before you jump in, let's cover some of the different tools you'll be using:\n",
    "\n",
    "+ [AI Platform](https://cloud.google.com/ai-platform) consists of tools that allow machine learning developers and data scientists to run their ML projects quickly and cost-effectively.\n",
    "\n",
    "+ [Cloud Storage](https://cloud.google.com/storage/) is a unified object storage for developers and enterprises, from live data serving to data analytics/ML to data archiving.\n",
    "\n",
    "+ [Cloud SDK](https://cloud.google.com/sdk/) is a command line tool which allows you to interact with Google Cloud products. This notebook introduces several `gcloud` and `gsutil` commands, which are part of the Cloud SDK. Note that shell commands in a notebook must be prepended with a `!`.\n",
    "\n",
    "+ [Pandas](https://pandas.pydata.org/) is a data analysis and manipulation tool built on top of the Python programming language.\n",
    "\n",
    "+ [scikit-learn](https://scikit-learn.org/stable/) is a machine learning and data analysis tool for the Python programming language that provides simple and efficient tools to analyze or predict data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your environment\n",
    "\n",
    "### Enable the required APIs\n",
    "\n",
    "In order to run this notebook successfully, confirm that the required API is enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable ml.googleapis.com\n",
    "!gcloud services enable bigquery.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citibike Dataset\n",
    "\n",
    "First, perform feature engineering on the Citibike dataset. This includes cleaning the data, extracting the necessary features, and transforming the data into feature columns.\n",
    "\n",
    "## Load the data\n",
    "\n",
    "### Import libraries\n",
    "Running the following cell will import the libraries needed to preprocess the Citibike dataset. \n",
    "\n",
    "+ `Pandas`: to store and manipulate the dataset\n",
    "* `Google Cloud Storage`: to retrieve the dataset from the GCS bucket where the dataset is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants\n",
    "Define the name of your Google Cloud Storage bucket where the cleaned data is stored. \n",
    "\n",
    "+ `BLOB_PREFIX`: indicates the folder where the files are stored\n",
    "+ `DIR_NAME`: holds the name of the local folder where the files will be downloaded to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'your-bucket-name'\n",
    "BLOB_PREFIX = 'clean_citibike_data2.csv.gz/part'\n",
    "DIR_NAME = 'citibike_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the files\n",
    "\n",
    "Run the following command to create a local folder where the dataset files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $DIR_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data cleaning job outputted multiple partioned files into the GCS bucket, you will need to loop through each file to access its contents. The following cell will retrieve all of the files with the `BLOB_PREFIX` defined above and download them. It will also create a list of the file names so they can be referenced later when loading the data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# List files in the bucket with the specified prefix\n",
    "blobs = storage_client.list_blobs(BUCKET_NAME, prefix=BLOB_PREFIX)\n",
    "\n",
    "# Go through the files and save them into the local folder\n",
    "filenames = []\n",
    "for i, blob in enumerate(blobs):\n",
    "    filename = f'{DIR_NAME}/citibike{i}.csv.gz'\n",
    "    blob.download_to_filename(filename)\n",
    "    filenames.append(filename)\n",
    "    print('Downloaded file: ' + str(blob.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the files into a dataframe\n",
    "\n",
    "Now, you can load the files into a dataframe. \n",
    "\n",
    "First, define the schema. From this dataset, you will need 4 columns:\n",
    "\n",
    "+ **starttime**: to extract the day of the week and date of when the trip starts\n",
    "+ **stoptime**: to extract the day of the week and date of when the trip has ended\n",
    "+ **start_station_id**: to find out how many trips started at a station\n",
    "+ **end_station_id**: to find out how many trips ended at a station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = (\n",
    "    'starttime',\n",
    "    'stoptime',\n",
    "    'start_station_id',\n",
    "    'end_station_id',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the following cell to loop through the files downloaded to the local folder, create a `Pandas DataFrame`, and view the first ten rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe\n",
    "citibike_data = pd.DataFrame()\n",
    "\n",
    "# For each file: load the contents into a dataframe\n",
    "# and concatenate the new dataframe with the existing\n",
    "for file in filenames:\n",
    "    print('Processing file: ' + file)\n",
    "    new_df = pd.read_csv(file, compression='gzip', usecols=[1, 2, 3, 7], header=None, \n",
    "                         names=COLUMNS, low_memory=False)\n",
    "    citibike_data = pd.concat([citibike_data, new_df])\n",
    "\n",
    "citibike_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "\n",
    "### Clean up the data\n",
    "The following cell will clean up the dataset in a few ways:\n",
    "\n",
    "+ Any rows with NAN values will be dropped\n",
    "+ The station IDs will be converted from floats to integers\n",
    "+ The times from the start time column will be removed since they are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NAN values\n",
    "citibike_data = citibike_data.dropna()\n",
    "\n",
    "# Convert station IDs to integers\n",
    "citibike_data['start_station_id'] = citibike_data['start_station_id'].astype('int32')\n",
    "citibike_data['end_station_id'] = citibike_data['end_station_id'].astype('int32')\n",
    "\n",
    "# Remove time from the time columns\n",
    "citibike_data['starttime'] = citibike_data['starttime'].apply(lambda t: t.split(\"T\")[0])\n",
    "citibike_data['stoptime'] = citibike_data['stoptime'].apply(lambda t: t.split(\"T\")[0])\n",
    "\n",
    "citibike_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count trips starting from a station\n",
    "\n",
    "Next, count the number of trips that have been taken from each station per day. The `groupby` function from `Pandas` will count the number of unique combinations of the start time and start station ID values. Then, the `pivot` function from `Pandas` can be used to convert the station IDs into columns (since they are the target data) and the counts as the values.\n",
    "\n",
    "Also, use the `add_suffix` function to rename the columns and distinguish that the values indicate trips that have started from the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique combinations of start time and start station ID values\n",
    "trips_started = (citibike_data.groupby(['starttime', 'start_station_id'])\n",
    "                              .size().reset_index().rename(columns={0: 'count'}))\n",
    "\n",
    "# Pivot to make station ID the columns and rename them\n",
    "trips_started = (trips_started.pivot(index='starttime', columns='start_station_id', values='count')\n",
    "                              .add_prefix('started_at_'))\n",
    "\n",
    "trips_started.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count trips ending at a station\n",
    "\n",
    "Running the following cell will repeat the same process as above, but will generate values for the number of trips that have ended at the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique combinations of start time and start station ID values\n",
    "trips_ended = (citibike_data.groupby(['stoptime', 'end_station_id'])\n",
    "                            .size().reset_index().rename(columns={0: 'count'}))\n",
    "\n",
    "# Pivot to make station ID the columns and rename them\n",
    "trips_ended = (trips_ended.pivot(index='stoptime', columns='end_station_id', values='count')\n",
    "                          .add_prefix('ending_at_'))\n",
    "\n",
    "trips_ended.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "The following cell will combine both dataframes for trips started and ended at the stations. Then, the NAN values will be filled as 0's since this indicates that no trips started or ended at the particular stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes\n",
    "# Set the index as row number instead of start time\n",
    "# Fill the NAN values with 0's\n",
    "citibike_df = (pd.concat([trips_started, trips_ended], axis=1)\n",
    "                .reset_index()\n",
    "                .fillna(0))\n",
    "\n",
    "# Rename the column with start and end dates\n",
    "citibike_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "citibike_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done with feature engineering for the Citibike Dataset! Now, you can move on to the external datasets you ingested in BigQuery to obtain more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gas Prices Dataset\n",
    "\n",
    "Now, perform feature engineering on the Gas Prices dataset. This includes cleaning the data, normalizing the price values, and transforming the data to match the Citibike dataset.\n",
    "\n",
    "## Load data\n",
    "\n",
    "### Import libraries\n",
    "Running the following cell will import the libraries needed to preprocess the Gas Prices dataset. \n",
    "\n",
    "+ `BigQuery`: to retrieve the dataset from BigQuery\n",
    "+ `Sklearn`: to normalize the gas prices values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from google.cloud import bigquery\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Run the following cell to load the Gas Prices dataset from BigQuery into a dataframe. It will define a query to select the columns needed from the gas prices dataset, run the query using the BigQuery client, and then convert it to a `Pandas DataFrame`.\n",
    "\n",
    "Make sure to fill in your project ID in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = \"US\"\n",
    "\n",
    "# Create the BigQuery client\n",
    "client = bigquery.Client(location=LOCATION)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "    SELECT Date as date, New_York_City_Average_USD_per_Gal as nyc_gas_price\n",
    "    FROM `your-project-id.new_york_citibike_trips.gas_prices`\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    location=LOCATION,\n",
    ")\n",
    "\n",
    "# Convert to a dataframe\n",
    "gas_df = query_job.to_dataframe()\n",
    "\n",
    "gas_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize values\n",
    "\n",
    "The gas price values range from around \\\\$2 to \\\\$5. It is important to normalize these values and scale them to be between 0 and 1 so that all the values within our dataset are weighted consistently. Running the following cell will create a scaler using the MinMaxScaler from scikit-learn and fit the gas prices to the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract gas prices column as a numpy array\n",
    "gas_values = gas_df[['nyc_gas_price']].values\n",
    "\n",
    "# Create scaler from sklearn\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Fit values to the scaler and replace column with normalized values\n",
    "gas_values_scaled = min_max_scaler.fit_transform(gas_values)\n",
    "gas_df['nyc_gas_price'] = gas_values_scaled\n",
    "\n",
    "gas_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy prices for the week\n",
    "\n",
    "The Citibike dataset contains values for each day, however, the Gas Prices dataset contains one value per week. To get values for each day, you can copy the price of the week's value for the entire seven days.\n",
    "\n",
    "First, run the following cell to refactor the date so it matches the format of a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def refactor_date(date):\n",
    "    '''Refactor the date strings so they match the Citibike dataset'''\n",
    "    parts = date.split('/')\n",
    "    return f'{parts[2]}-{parts[0]}-{parts[1]}'\n",
    "\n",
    "gas_df['date'] = gas_df['date'].apply(lambda d: refactor_date(d))\n",
    "gas_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, copy the gas price of one day for the whole week by adding new rows to the dataframe.\n",
    "\n",
    "The following cell does this by applying a function to each row in the dataframe that:\n",
    "+ Converts each date to a datetime object\n",
    "+ Loops through the next six days to create new rows\n",
    "+ Appending the new row to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list to hold new rows\n",
    "new_rows = []\n",
    "\n",
    "def copy_values_for_week(row):\n",
    "    '''Copies gas price of one day for the entire week '''\n",
    "    today = datetime.datetime.strptime(row['date'], '%Y-%m-%d')\n",
    "    # Loop through the next six days\n",
    "    for j in range(1,7):  \n",
    "        # Create and a new row for the next day\n",
    "        new_day = datetime.datetime.strftime(today + datetime.timedelta(days=j), '%Y-%m-%d')\n",
    "        new_row = {'date': new_day, 'nyc_gas_price': row['nyc_gas_price']}\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Apply copy function to dataframe\n",
    "gas_df.apply(copy_values_for_week, axis=1)\n",
    "\n",
    "# Add new rows to dataframe\n",
    "gas_df = gas_df.append(new_rows)\n",
    "\n",
    "gas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the datasets\n",
    "\n",
    "Now that the Gas Prices dataset is transformed, you can combine it with the Citibike dataset. Run the following cell to combine both dataframes. It will also drop rows with NAN values so that each row has all the feature columns filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge both dataframes based on the date common column\n",
    "# Drop rows with NAN values\n",
    "final_df = pd.merge(gas_df, citibike_df, on=\"date\").dropna()\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve the date feature\n",
    "\n",
    "Now that all the datasets have been combined, you can separate the start time column into more features such as the year, month, and day. Then, the date column can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name and year, month, and day columns\n",
    "date_columns = final_df['date'].str.split('-', expand=True)\n",
    "date_names = ['year', 'month', 'day']\n",
    "\n",
    "# Add the columns at the start of the dataset\n",
    "for i in range(3):\n",
    "    final_df.insert(0, date_names[i], date_columns[i])\n",
    "    final_df[date_names[i]] = final_df[date_names[i]].astype('int32')\n",
    "\n",
    "# Remove the date column from the dataframe\n",
    "final_df = final_df.drop('date', axis=1)\n",
    "\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will extract the day of the week from the date information using the `Datetime` python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weekday(df):\n",
    "    ''' Creates a datetime object and returns the day of the week '''\n",
    "    date = datetime.datetime(int(df['year']), int(df['month']), int(df['day']))\n",
    "    return date.weekday()\n",
    "\n",
    "# Apply the find_weekday() function to every row of the dataset\n",
    "weekday_col = final_df.apply(find_weekday, axis=1)\n",
    "\n",
    "# Insert the weekday column at the start\n",
    "final_df.insert(0, 'weekday', weekday_col)\n",
    "\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
