{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering with Pandas\n",
    "\n",
    "This notebook demonstrates how to use AI Platform notebooks to perform feature engineering on a dataset using `pandas`.\n",
    "\n",
    "You will load the data into a `Pandas DataFrame`, clean up the columns into a usable format, and then restructure the data into feature and target data columns. \n",
    "\n",
    "Before you jump in, let's cover some of the different tools you'll be using:\n",
    "\n",
    "+ [AI Platform](https://cloud.google.com/ai-platform) consists of tools that allow machine learning developers and data scientists to run their ML projects quickly and cost-effectively.\n",
    "\n",
    "+ [Cloud Storage](https://cloud.google.com/storage/) is a unified object storage for developers and enterprises, from live data serving to data analytics/ML to data archiving.\n",
    "\n",
    "+ [Cloud SDK](https://cloud.google.com/sdk/) is a command line tool which allows you to interact with Google Cloud products. This notebook introduces several `gcloud` and `gsutil` commands, which are part of the Cloud SDK. Note that shell commands in a notebook must be prepended with a `!`.\n",
    "\n",
    "+ [Pandas](https://pandas.pydata.org/) is a data analysis and manipulation tool built on top of the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up your environment\n",
    "\n",
    "### Enable the required APIs\n",
    "\n",
    "In order to use AI Platform, confirm that the required API is enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable ml.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "## Import libraries\n",
    "Running the following cell will import the libraries needed. \n",
    "\n",
    "+ `Pandas`: to store and manipulate the dataset.\n",
    "* `Google Cloud Storage`: to retrieve the dataset from the GCS bucket where the dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants\n",
    "Define the name of your Google Cloud Storage bucket where the cleaned data is stored. \n",
    "\n",
    "+ `BLOB_PREFIX`: indicates the folder where the files are stored.\n",
    "+ `DIR_NAME`: holds the name of the local folder where the files will be downloaded to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'your-bucket-name'\n",
    "BLOB_PREFIX = 'clean_citibike_data.csv.gz/part'\n",
    "DIR_NAME = 'citibike_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Run the following command to create a local folder where the dataset files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $DIR_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data cleaning job outputted multiple partioned files into the GCS bucket, you will need to loop through each file to access its contents. The following cell will retrieve all of the files with the `BLOB_PREFIX` defined above and download them. It will also create a list of the file names so they can be referenced later when loading the data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# List files in the bucket with the specified prefix\n",
    "blobs = storage_client.list_blobs(BUCKET_NAME, prefix=BLOB_PREFIX)\n",
    "\n",
    "# Go through the files and save them into the local folder\n",
    "filenames = []\n",
    "for i, blob in enumerate(blobs):\n",
    "    filename = f'{DIR_NAME}/citibike{i}.csv.gz'\n",
    "    blob.download_to_filename(filename)\n",
    "    filenames.append(filename)\n",
    "    print('Downloaded file: ' + str(blob.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can load the files into a dataframe. \n",
    "\n",
    "First, define the schema. From this dataset, you will need 4 columns:\n",
    "\n",
    "+ **starttime**: to extract the day of the week and date of when the trip starts\n",
    "+ **stoptime**: to extract the day of the week and date of when the trip has ended\n",
    "+ **start_station_id**: to find out how many trips started at a station\n",
    "+ **end_station_id**: to find out how many trips ended at a station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = (\n",
    "    'starttime',\n",
    "    'stoptime',\n",
    "    'start_station_id',\n",
    "    'end_station_id',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the following cell to loop through the files downloaded to the local folder, create a `Pandas DataFrame`, and view the first ten rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe\n",
    "training_data = pd.DataFrame()\n",
    "\n",
    "# Loop through the files\n",
    "# For each file: load the contents into a dataframe, concatenate the new dataframe with the existing\n",
    "for file in filenames:\n",
    "    print('Processing file: ' + file)\n",
    "    training_data = pd.concat([training_data, pd.read_csv(file, compression='gzip', usecols=[1, 2, 3, 7], header=None, names=COLUMNS, low_memory=False)])\n",
    "\n",
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features\n",
    "\n",
    "The following cell will clean up the dataset in a few ways:\n",
    "\n",
    "+ Any rows with NAN values will be dropped\n",
    "+ The station IDs will be converted from floats to integers\n",
    "+ The times from the start time column will be removed since they are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NAN values\n",
    "training_data = training_data.dropna()\n",
    "\n",
    "# Convert station IDs to integers\n",
    "training_data['start_station_id'] = training_data['start_station_id'].astype('int32')\n",
    "training_data['end_station_id'] = training_data['end_station_id'].astype('int32')\n",
    "\n",
    "# Remove time from the time columns\n",
    "training_data['starttime'] = training_data['starttime'].apply(lambda t: t.split(\"T\")[0])\n",
    "training_data['stoptime'] = training_data['stoptime'].apply(lambda t: t.split(\"T\")[0])\n",
    "\n",
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will count the number of trips that have been taken from each station per day. The `groupby` function from `Pandas` will count the number of unique combinations of the start time and start station ID values. Then, the `pivot` function from `Pandas` can be used to convert the station IDs into columns (since they are the target data) and the counts as the values.\n",
    "\n",
    "You will also use the `add_suffix` function to rename the columns and distinguish that the values indicate trips that have started from the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique combinations of start time and start station ID values\n",
    "bikes_taken = training_data.groupby(['starttime', 'start_station_id']).size().reset_index().rename(columns={0: 'count'})\n",
    "\n",
    "# Pivot to make station ID the columns and rename them\n",
    "bikes_taken = bikes_taken.pivot(index='starttime', columns='start_station_id', values='count').add_prefix('started_at_')\n",
    "\n",
    "bikes_taken.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following cell will repeat the same process as above, but will generate values for the number of trips that have ended at the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique combinations of start time and start station ID values\n",
    "bikes_deposit = training_data.groupby(['stoptime', 'end_station_id']).size().reset_index().rename(columns={0: 'count'})\n",
    "\n",
    "# Pivot to make station ID the columns and rename them\n",
    "bikes_deposit = bikes_deposit.pivot(index='stoptime', columns='end_station_id', values='count').add_prefix('ending_at')\n",
    "\n",
    "bikes_deposit.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will combine both dataframes for bikes taken and deposited. Then, the NAN values will be filled as 0's since this indicates that no trips started or ended at the particular stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes\n",
    "# Set the index as row number instead of start time\n",
    "# Fill the NAN values with 0's\n",
    "training_df = pd.concat([bikes_taken, bikes_deposit], axis=1)\\\n",
    "                .reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill='')\\\n",
    "                .fillna(0)\n",
    "\n",
    "# Rename the column with start and end dates\n",
    "training_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can separate the start time column into more features such as the year, month, and day. Then, the date column can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name and year, month, and day columns\n",
    "date_columns = training_df['date'].str.split('-', expand=True)\n",
    "date_names = ['year', 'month', 'day']\n",
    "\n",
    "# Add the columns at the start of the dataset\n",
    "for i in range(3):\n",
    "    training_df.insert(0, date_names[i], date_columns[i])\n",
    "    training_df[date_names[i]] = training_df[date_names[i]].astype('int32')\n",
    "\n",
    "# Remove the date column from the dataframe\n",
    "training_df = training_df.drop('date', axis=1)\n",
    "\n",
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will extract the day of the week from the date information using the `Datetime` python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def find_weekday(df):\n",
    "    ''' Creates a datetime object and returns the day of the week '''\n",
    "    date = datetime.datetime(int(df['year']), int(df['month']), int(df['day']))\n",
    "    return date.weekday()\n",
    "\n",
    "# Apply the find_weekday() function to every row of the dataset\n",
    "weekday_col = training_df.apply(find_weekday, axis=1)\n",
    "\n",
    "# Insert the weekday column at the start\n",
    "training_df.insert(0, 'weekday', weekday_col)\n",
    "\n",
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done with feature engineering for the Citibike Dataset! Now, you can move on to the external datasets you ingested in BigQuery to obtain more features."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
