{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering using AutoML Tables\n",
    "\n",
    "## Overview\n",
    "In this notebook we will see how [AutoML Tables](https://cloud.google.com/automl-tables/) can be used to make music recommendations to users using collaborative filtering. AutoML Tables is a supervised learning service for structured data that can vastly simplify the model building process.\n",
    "\n",
    "### Dataset\n",
    "AutoML Tables allows data to be imported from either GCS or BigQuery. This tutorial uses the [ListenBrainz](https://console.cloud.google.com/marketplace/details/metabrainz/listenbrainz) dataset from [Cloud Marketplace](https://console.cloud.google.com/marketplace), hosted in BigQuery.\n",
    "\n",
    "The ListenBrainz dataset is a log of songs played by users, some notable pieces of the schema include:\n",
    "  - **user_name:** a user id.\n",
    "  - **track_name:** a song id.\n",
    "  - **artist_name:** the artist of the song.\n",
    "  - **release_name:** the album of the song.\n",
    "  - **tags:** the genres of the song.\n",
    "\n",
    "### Objective\n",
    "The goal of this notebook is to demonstrate how to create a lookup table in BigQuery of songs to recommend to users using a log of user-song listens and AutoML Tables. This will be done by training a regression model to predict how similar a given `user` and `song` are on a 0 to 1 scale, and using predictions for every `(user, song)` pair to generate a ranking of the most similar songs for each user.\n",
    "\n",
    "As the number of `(user, song)` pairs grows exponentially with the number of unique users and songs, this approach may not be optimal for extremely large datasets. One workaround would be to train a model that learns to embed users and songs in the same embedding space, and use a nearest-neighbors algorithm to get recommendations for users. Unfortunately, AutoML Tables does not expose any feature for training and using embeddings, so a [custom ML model](https://github.com/GoogleCloudPlatform/professional-services/tree/master/examples/cloudml-collaborative-filtering) would need to be used instead.\n",
    "\n",
    "Another recommendation approach that is worth mentioning is [using extreme multiclass classification](https://ai.google/research/pubs/pub45530), as that also circumvents storing every possible pair of users and songs. Unfortunately, AutoML Tables does not support the multiclass classification of more than [100 classes](https://cloud.google.com/automl-tables/docs/prepare#target-requirements).\n",
    "\n",
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud Platform (GCP):\n",
    "- Cloud AutoML Tables\n",
    "\n",
    "Learn about [AutoML Tables pricing](https://cloud.google.com/automl-tables/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [AutoML Tables documentation](https://cloud.google.com/automl-tables/docs/) to\n",
    "* [Enable billing](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "* [Enable AutoML API](https://console.cloud.google.com/apis/library/automl.googleapis.com?q=automl&project=automl-music-recommendation&folder&organizationId=433637338589)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 PIP Install Packages and dependencies\n",
    "Install addional dependencies not installed in the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet google-cloud-automl google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the kernel to allow `automl_v1beta1` to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the following cell with the necessary constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GCP project id.\n",
    "PROJECT_ID = \"\"\n",
    "# The region to use for compute resources.\n",
    "LOCATION = \"\"\n",
    "# A name for the AutoML tables Dataset to create.\n",
    "DATASET_DISPLAY_NAME = \"\"\n",
    "# The BigQuery dataset to import data from (doesn't need to exist).\n",
    "INPUT_BQ_DATASET = \"\"\n",
    "# The BigQuery table to import data from (doesn't need to exist).\n",
    "INPUT_BQ_TABLE = \"\"\n",
    "# A name for the AutoML tables model to create.\n",
    "MODEL_DISPLAY_NAME = \"\"\n",
    "# The number of hours to train the model.\n",
    "MODEL_TRAIN_HOURS = 0\n",
    "\n",
    "assert all([\n",
    "    PROJECT_ID,\n",
    "    LOCATION,\n",
    "    DATASET_DISPLAY_NAME,\n",
    "    INPUT_BQ_DATASET,\n",
    "    INPUT_BQ_TABLE,\n",
    "    MODEL_DISPLAY_NAME,\n",
    "    MODEL_TRAIN_HOURS,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages and initialize clients for BigQuery and AutoML Tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/automl-music-recommendation/locations/us-central1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from google.cloud import automl_v1beta1\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "aml_client = automl_v1beta1.AutoMlClient()\n",
    "prediction_client = automl_v1beta1.PredictionServiceClient()\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "location_path = aml_client.location_path(PROJECT_ID, LOCATION)\n",
    "location_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a model, a structured dataset must be injested into AutoML tables from either BigQuery or Google Cloud Storage. Once injested, the user will be able to cherry pick columns to use as features, labels, or weights and configure the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create BigQuery table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, do some feature engineering on the original ListenBrainz dataset to turn it into a dataset for training and export it into a seperate BigQuery table:\n",
    "\n",
    "    1. Make each sample a unique `(user, song)` pair.\n",
    "    2. For features, use the user's top 10 songs ever played and the song's number of albums, artist, and genres.\n",
    "    3. For a label, use the number of times the user has listened to the song, normalized by dividing by the maximum number of times that user has listened to any song. Normalizing the listen counts ensures active users don't have disproportionate effect on the model error.\n",
    "    4. Add a weight equal to the label to give songs more popular with the user higher weights. This is to help account for the skew in the label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "  WITH\n",
    "    songs AS (\n",
    "      SELECT CONCAT(track_name, \" by \", artist_name) AS song\n",
    "      FROM `listenbrainz.listenbrainz.listen`\n",
    "      GROUP BY song\n",
    "      ORDER BY COUNT(*) DESC\n",
    "      LIMIT 10000\n",
    "    ),\n",
    "    user_songs AS (\n",
    "      SELECT user_name AS user, ANY_VALUE(artist_name) AS artist,\n",
    "        CONCAT(track_name, \" by \", artist_name) AS song,\n",
    "        COUNT(*) AS user_song_listens\n",
    "      FROM `listenbrainz.listenbrainz.listen`\n",
    "      JOIN songs ON songs.song = CONCAT(track_name, \" by \", artist_name)\n",
    "      WHERE track_name != \"\"\n",
    "      GROUP BY user_name, song\n",
    "    ),\n",
    "    user_song_ranks AS (\n",
    "      SELECT user, song, user_song_listens,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user ORDER BY user_song_listens DESC)\n",
    "          AS rank\n",
    "      FROM user_songs\n",
    "    ),\n",
    "    user_features AS (\n",
    "      SELECT user, ARRAY_AGG(song) AS top_10,\n",
    "        MAX(user_song_listens) AS user_max_listen\n",
    "      FROM user_song_ranks\n",
    "      WHERE rank <= 10\n",
    "      GROUP BY user\n",
    "    ),\n",
    "    item_features AS (\n",
    "      SELECT CONCAT(track_name, \" by \", artist_name) AS song,\n",
    "        SPLIT(ANY_VALUE(tags), \",\") AS tags,\n",
    "        COUNT(DISTINCT(release_name)) AS albums\n",
    "      FROM `listenbrainz.listenbrainz.listen`\n",
    "      WHERE track_name != \"\"\n",
    "      GROUP BY song\n",
    "    )\n",
    "  SELECT user, song, artist, tags, albums, top_10,\n",
    "    user_song_listens/user_max_listen AS count_norm,\n",
    "    SQRT(user_song_listens/user_max_listen) AS weight\n",
    "  FROM user_songs\n",
    "  JOIN user_features USING(user)\n",
    "  JOIN item_features USING(song)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_from_query(query, table):\n",
    "    \"\"\"Creates a new table using the results from the given query.\n",
    "    \n",
    "    Args:\n",
    "        query: a query string.\n",
    "        table: a name to give the new table.\n",
    "    \"\"\"\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    bq_dataset = bigquery.Dataset(\"{0}.{1}\".format(PROJECT_ID, INPUT_BQ_DATASET))\n",
    "    bq_dataset.location = \"US\"\n",
    "\n",
    "    try:\n",
    "        bq_dataset = bq_client.create_dataset(bq_dataset)\n",
    "    except exceptions.Conflict:\n",
    "        pass\n",
    "\n",
    "    table_ref = bq_client.dataset(INPUT_BQ_DATASET).table(table)\n",
    "    job_config.destination = table_ref\n",
    "\n",
    "    query_job = bq_client.query(query,\n",
    "                             location=bq_dataset.location,\n",
    "                             job_config=job_config)\n",
    "\n",
    "    query_job.result()\n",
    "    print('Query results loaded to table {}'.format(table_ref.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results loaded to table /projects/automl-music-recommendation/datasets/notebook_dataset/tables/training_data\n"
     ]
    }
   ],
   "source": [
    "create_table_from_query(query, INPUT_BQ_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create AutoML Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset by importing the BigQuery table that was just created. Importing data may take a few minutes or hours depending on the size of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_config = {\n",
    "    \"display_name\": DATASET_DISPLAY_NAME,\n",
    "    \"tables_dataset_metadata\": {},\n",
    "}\n",
    "dataset = aml_client.create_dataset(location_path, dataset_config)\n",
    "\n",
    "dataset_bq_input_uri = 'bq://{0}.{1}.{2}'.format(PROJECT_ID, INPUT_BQ_DATASET, INPUT_BQ_TABLE)\n",
    "input_config = {\n",
    "    'bigquery_source': {\n",
    "        'input_uri': dataset_bq_input_uri\n",
    "    }\n",
    "}\n",
    "import_data_response = aml_client.import_data(dataset.name, input_config)\n",
    "import_data_result = import_data_response.result()\n",
    "import_data_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the datatypes assigned to each column. In this case, the `song` and `artist` should be categorical, not textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user', 'CATEGORY'),\n",
       " ('weight', 'FLOAT64'),\n",
       " ('albums', 'FLOAT64'),\n",
       " ('tags', 'ARRAY'),\n",
       " ('count_norm', 'FLOAT64'),\n",
       " ('song', 'STRING'),\n",
       " ('artist', 'STRING'),\n",
       " ('top_10', 'ARRAY')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_table_specs_response = aml_client.list_table_specs(dataset.name)\n",
    "table_specs = [s for s in list_table_specs_response]\n",
    "table_spec_name = table_specs[0].name\n",
    "list_column_specs_response = aml_client.list_column_specs(table_spec_name)\n",
    "column_specs = {s.display_name: s for s in list_column_specs_response}\n",
    "\n",
    "def print_column_specs(column_specs):\n",
    "    \"\"\"Parses the given specs and prints each column and column type.\"\"\"\n",
    "    data_types = automl_v1beta1.proto.data_types_pb2\n",
    "    return [(x, data_types.TypeCode.Name(\n",
    "        column_specs[x].data_type.type_code)) for x in column_specs.keys()]\n",
    "\n",
    "print_column_specs(column_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Update Dataset params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the types AutoML Tables automatically assigns each column will be off from that they were intended to be. When that happens, we need to update Tables with different types for certain columns.\n",
    "\n",
    "In this case, set the `song` and `artist` column types to `CATEGORY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user', 'CATEGORY'),\n",
       " ('weight', 'FLOAT64'),\n",
       " ('albums', 'FLOAT64'),\n",
       " ('tags', 'ARRAY'),\n",
       " ('count_norm', 'FLOAT64'),\n",
       " ('song', 'CATEGORY'),\n",
       " ('artist', 'CATEGORY'),\n",
       " ('top_10', 'ARRAY')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in [\"song\", \"artist\"]:\n",
    "    update_column_spec_dict = {\n",
    "        \"name\": column_specs[col].name,\n",
    "        \"data_type\": {\n",
    "            \"type_code\": \"CATEGORY\"\n",
    "        }\n",
    "    }\n",
    "    aml_client.update_column_spec(update_column_spec_dict)\n",
    "\n",
    "list_column_specs_response = aml_client.list_column_specs(table_spec_name)\n",
    "column_specs = {s.display_name: s for s in list_column_specs_response}\n",
    "print_column_specs(column_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all columns are feature columns, in order to train a model, we need to tell Tables which column should be used as the target variable and, optionally, which column should be used as sample weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/120451501752/locations/us-central1/datasets/TBL4001333919709396992\"\n",
       "display_name: \"listenbrainz\"\n",
       "create_time {\n",
       "  seconds: 1562105555\n",
       "  nanos: 279259000\n",
       "}\n",
       "etag: \"AB3BwFquIN-yc6mMK2rEEGAcNy-Ctwb-SplDYyvFHJm798sET7xJHC0FwK4zrg7LLLc=\"\n",
       "example_count: 2467282\n",
       "tables_dataset_metadata {\n",
       "  primary_table_spec_id: \"6310827307527307264\"\n",
       "  target_column_spec_id: \"6309217622504243200\"\n",
       "  weight_column_spec_id: \"7462139127111090176\"\n",
       "  stats_update_time {\n",
       "    seconds: 1562105803\n",
       "    nanos: 263000000\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_column_name = \"count_norm\"\n",
    "label_column_spec = column_specs[label_column_name]\n",
    "label_column_id = label_column_spec.name.rsplit('/', 1)[-1]\n",
    "\n",
    "weight_column_name = \"weight\"\n",
    "weight_column_spec = column_specs[weight_column_name]\n",
    "weight_column_id = weight_column_spec.name.rsplit('/', 1)[-1]\n",
    "\n",
    "update_dataset_dict = {\n",
    "    'name': dataset.name,\n",
    "    'tables_dataset_metadata': {\n",
    "        'target_column_spec_id': label_column_id,\n",
    "        'weight_column_spec_id': weight_column_id,\n",
    "    }\n",
    "}\n",
    "update_dataset_response = aml_client.update_dataset(update_dataset_dict)\n",
    "update_dataset_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Dataset has been configured correctly, we can tell AutoML Tables to train a new model. The amount of resources spent to train this model can be adjusted using a parameter called `train_budget_milli_node_hours`. As the name implies, this puts a maximum budget on how many resources a training job can use up before exporting a servable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/120451501752/locations/us-central1/models/TBL5087774553254395904\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_list = list(column_specs.keys())\n",
    "feat_list.remove(label_column_name)\n",
    "feat_list.remove(weight_column_name)\n",
    "\n",
    "model_dict = {\n",
    "    'display_name': MODEL_DISPLAY_NAME,\n",
    "    'dataset_id': dataset.name.rsplit('/', 1)[-1],\n",
    "    'tables_model_metadata': {\n",
    "        'train_budget_milli_node_hours': MODEL_TRAIN_HOURS * 1000,\n",
    "        'target_column_spec': column_specs[label_column_name],\n",
    "        'input_feature_column_specs': [column_specs[x] for x in feat_list]\n",
    "    },\n",
    "}\n",
    "    \n",
    "create_model_response = aml_client.create_model(location_path, model_dict)\n",
    "create_model_result = create_model_response.result()\n",
    "create_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are optimizing a surrogate problem (predicting the similarity between `(user, song)` pairs) in order to achieve our final objective of producing a list of recommended songs for a user, it's difficult to tell how well the model performs by looking only at the final loss function. Instead, an evaluation metric we can use for our model is `recall@n` for the top `m` most listened to songs for each user. This metric will give us the probability that one of a user's top `m` most listened to songs will appear in the top `n` recommendations we make.\n",
    "\n",
    "In order to get the top recommendations for each user, we need to create a batch job to predict similarity scores between each user and item pair. These similarity scores would then be sorted per user to produce an ordered list of recommended songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create an evaluation table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of creating a lookup table for all users, let's just focus on the performance for a single user for this tutorial. We will make recommendations specifically for the user `rob`. We start by creatings a dataset for prediction to feed into the trained model; this is a table of every possible `(user, song)` pair containing `rob` and corresponding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"rob\"\n",
    "training_table = \"{}.{}.{}\".format(PROJECT_ID, INPUT_BQ_DATASET, INPUT_BQ_TABLE)\n",
    "query = \"\"\"\n",
    "    WITH\n",
    "      pairs AS (\n",
    "        SELECT \"{0}\" AS user, song, ANY_VALUE(artist) as artist,\n",
    "          ANY_VALUE(tags) as tags, ANY_VALUE(albums) as albums\n",
    "        FROM `{1}`\n",
    "        GROUP BY song\n",
    "      ),\n",
    "      user_features AS (\n",
    "        SELECT user, ANY_VALUE(top_10) as top_10\n",
    "        FROM `{1}`\n",
    "        GROUP BY user\n",
    "      )\n",
    "    SELECT * FROM pairs\n",
    "    JOIN user_features USING(user)\n",
    "\"\"\".format(user, training_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results loaded to table /projects/automl-music-recommendation/datasets/notebook_dataset/tables/training_data_example\n"
     ]
    }
   ],
   "source": [
    "eval_table = \"{}_example\".format(INPUT_BQ_TABLE)\n",
    "create_table_from_query(query, eval_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the prediction table is created, start a batch prediction job. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = aml_client.model_path(PROJECT_ID, LOCATION, create_model_result.name)\n",
    "preds_bq_input_uri = \"bq://{}.{}.{}\".format(PROJECT_ID, INPUT_BQ_DATASET, eval_table)\n",
    "preds_bq_output_uri = \"bq://{}\".format(PROJECT_ID)\n",
    "\n",
    "input_config = {\"bigquery_source\": {\"input_uri\": preds_bq_input_uri}}\n",
    "output_config = {\"bigquery_destination\": {\"output_uri\": preds_bq_output_uri}}\n",
    "response = prediction_client.batch_predict(create_model_result.name, input_config, output_config)\n",
    "response.result()\n",
    "output_uri = response.metadata.batch_predict_details.output_info.bigquery_output_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the similarity predictions for `rob`, we can order by the predictions to get a ranked list of songs to recommend to `rob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 songs recommended for rob:\n",
      "1. Battle Against Time by Wintersun\n",
      "2. Shelter by Porter Robinson\n",
      "3. Cirice by Ghost\n",
      "4. God's Plan by Drake\n",
      "5. Schism by Tool\n",
      "6. Intro by alt-J\n",
      "7. Vicarious by Tool\n",
      "8. The Grudge by Tool\n",
      "9. IV. Sweatpants by Childish Gambino\n",
      "10. Down With the Sun by Insomnium\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "query = \"\"\"\n",
    "    SELECT user, song, tables.value as pred, count_norm as label\n",
    "    FROM `{}.predictions` a, UNNEST(predicted_count_norm)\n",
    "    LEFT JOIN `{}` USING(user, song)\n",
    "    WHERE user = \"{}\"\n",
    "    ORDER BY pred DESC\n",
    "    LIMIT {}\n",
    "\"\"\".format(output_uri[5:].replace(\":\", \".\"), training_table, user, n)\n",
    "query_job = bq_client.query(query)\n",
    "\n",
    "print(\"Top {} songs recommended for {}:\".format(n, user))\n",
    "for idx, row in enumerate(query_job):\n",
    "    print(\"{}.\".format(idx + 1), row[\"song\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate `recall@n`, we need to join in the ground truth similarities for all songs `rob` has already listened to. With this additional data, we can find the top `n` songs that would be recommended and see how many of `rob`s top `m` songs appear in that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of user's top 100 songs in recommended top 1000: 0.3\n"
     ]
    }
   ],
   "source": [
    "user_top_n = 100\n",
    "recall_n = 1000\n",
    "query = \"\"\"\n",
    "    WITH\n",
    "      top_k AS (\n",
    "        SELECT user, song, count_norm,\n",
    "          ROW_NUMBER() OVER (PARTITION BY user ORDER BY count_norm DESC) as user_rank\n",
    "        FROM `{0}`\n",
    "      ),\n",
    "      preds AS (\n",
    "        SELECT user, song, tables.value as pred, count_norm as label,\n",
    "          ROW_NUMBER() OVER (ORDER BY tables.value DESC) as rank, user_rank\n",
    "        FROM `{1}.predictions` a, UNNEST(predicted_count_norm)\n",
    "        LEFT JOIN top_k USING(user, song)\n",
    "        ORDER BY pred DESC\n",
    "      )\n",
    "    SELECT COUNT(label)/{2} as recall_{3}_top_{2}\n",
    "    FROM preds\n",
    "    WHERE rank <= {3} AND user_rank <= {2}\n",
    "\"\"\".format(training_table, output_uri[5:].replace(\":\", \".\"), user_top_n, recall_n)\n",
    "query_job = bq_client.query(query)\n",
    "\n",
    "for row in query_job:\n",
    "    print(\"Recall of user's top {} songs in recommended top {}: {}\".format(\n",
    "        user_top_n, recall_n, row[\"recall_{}_top_{}\".format(recall_n, user_top_n)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cells to clean up the BigQuery tables and AutoML Table Datasets that were created with this notebook to avoid additional charges for storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Delete the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.api_core.operation.Operation at 0x7f2ab5ce2c88>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aml_client.delete_dataset(dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Delete BigQuery datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to delete BigQuery tables, make sure the service account linked to this notebook has a role with the `bigquery.tables.delete` permission such as `Big Query Data Owner`. The following command displays the current service account.\n",
    "\n",
    "IAM permissions can be adjusted [here](https://console.cloud.google.com/iam-admin/iam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120451501752-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list account --format \"value(core.account)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the BigQuery tables created by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete the prediction dataset.\n",
    "# dataset_id = str(output_uri[5:].replace(\":\", \".\"))\n",
    "# bq_client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n",
    "\n",
    "# # Delete the training dataset.\n",
    "# dataset_id = \"{0}.{1}\".format(PROJECT_ID, INPUT_BQ_DATASET)\n",
    "# bq_client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
