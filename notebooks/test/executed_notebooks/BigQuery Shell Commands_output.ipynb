{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery command-line tool\n",
    "\n",
    "The BigQuery command-line tool is installed as part of the [Cloud SDK](https://cloud-dot-devsite.googleplex.com/sdk/docs/), and can be used to interact with BigQuery using shell commands instead of Python code. Note that shell commands in a notebook must be prepended with a `!`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View available commands\n",
    "\n",
    "To view the available commands for the BigQuery command-line tool, use the `--help` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python script for interacting with BigQuery.\r\n",
      "\r\n",
      "\r\n",
      "USAGE: bq [--global_flags] <command> [--command_flags] [args]\r\n",
      "\r\n",
      "\r\n",
      "Any of the following commands:\r\n",
      "  cancel, cp, extract, head, help, init, insert, load, ls, mk, mkdef, partition,\r\n",
      "  query, rm, shell, show, update, version, wait\r\n",
      "\r\n",
      "\r\n",
      "cancel     Request a cancel and waits for the job to be cancelled.\r\n",
      "\r\n",
      "           Requests a cancel and then either: a) waits until the job is done if\r\n",
      "           the sync flag is set [default], or b) returns immediately if the sync\r\n",
      "           flag is not set. Not all job types support a cancel, an error is\r\n",
      "           returned if it cannot be cancelled. Even for jobs that support a\r\n",
      "           cancel, success is not guaranteed, the job may have completed by the\r\n",
      "           time the cancel request is noticed, or the job may be in a stage\r\n",
      "           where it cannot be cancelled.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq cancel job_id # Requests a cancel and waits until the job is done.\r\n",
      "           bq --nosync cancel job_id # Requests a cancel and returns\r\n",
      "           immediately.\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           job_id: Job ID to cancel.\r\n",
      "\r\n",
      "cp         Copies one table to another.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq cp dataset.old_table dataset2.new_table\r\n",
      "           bq cp --destination_kms_key=kms_key dataset.old_table\r\n",
      "           dataset2.new_table\r\n",
      "\r\n",
      "extract    Perform an extract operation of source_table into destination_uris.\r\n",
      "\r\n",
      "           Usage:\r\n",
      "           extract <source_table> <destination_uris>\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq extract ds.summary gs://mybucket/summary.csv\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           source_table: Source table to extract.\r\n",
      "           destination_uris: One or more Google Cloud Storage URIs, separated by\r\n",
      "           commas.\r\n",
      "\r\n",
      "head       Displays rows in a table.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq head dataset.table\r\n",
      "           bq head -j job\r\n",
      "           bq head -n 10 dataset.table\r\n",
      "           bq head -s 5 -n 10 dataset.table\r\n",
      "\r\n",
      "help       Help for all or selected command:\r\n",
      "               bq help [<command>]\r\n",
      "\r\n",
      "           To retrieve help with global flags:\r\n",
      "               bq --help\r\n",
      "\r\n",
      "           To retrieve help with flags only from the main module:\r\n",
      "               bq --helpshort [<command>]\r\n",
      "\r\n",
      "init       Authenticate and create a default .bigqueryrc file.\r\n",
      "\r\n",
      "insert     Inserts rows in a table.\r\n",
      "\r\n",
      "           Inserts the records formatted as newline delimited JSON from file\r\n",
      "           into the specified table. If file is not specified, reads from stdin.\r\n",
      "           If there were any insert errors it prints the errors to stdout.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq insert dataset.table /tmp/mydata.json\r\n",
      "           echo '{\"a\":1, \"b\":2}' | bq insert dataset.table\r\n",
      "\r\n",
      "           Template table examples: Insert to dataset.template_suffix table\r\n",
      "           using dataset.template table as its template.\r\n",
      "           bq insert -x=_suffix dataset.table /tmp/mydata.json\r\n",
      "\r\n",
      "load       Perform a load operation of source into destination_table.\r\n",
      "\r\n",
      "           Usage:\r\n",
      "           load <destination_table> <source> [<schema>]\r\n",
      "\r\n",
      "           The <destination_table> is the fully-qualified table name of table to\r\n",
      "           create, or append to if the table already exists.\r\n",
      "\r\n",
      "           The <source> argument can be a path to a single local file, or a\r\n",
      "           comma-separated list of URIs.\r\n",
      "\r\n",
      "           The <schema> argument should be either the name of a JSON file or a\r\n",
      "           text schema. This schema should be omitted if the table already has\r\n",
      "           one.\r\n",
      "\r\n",
      "           In the case that the schema is provided in text form, it should be a\r\n",
      "           comma-separated list of entries of the form name[:type], where type\r\n",
      "           will default to string if not specified.\r\n",
      "\r\n",
      "           In the case that <schema> is a filename, it should contain a single\r\n",
      "           array object, each entry of which should be an object with properties\r\n",
      "           'name', 'type', and (optionally) 'mode'. See the online documentation\r\n",
      "           for more detail:\r\n",
      "           https://developers.google.com/bigquery/preparing-data-for-bigquery\r\n",
      "\r\n",
      "           Note: the case of a single-entry schema with no type specified is\r\n",
      "           ambiguous; one can use name:string to force interpretation as a\r\n",
      "           text schema.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq load ds.new_tbl ./info.csv ./info_schema.json\r\n",
      "           bq load ds.new_tbl gs://mybucket/info.csv ./info_schema.json\r\n",
      "           bq load ds.small gs://mybucket/small.csv name:integer,value:string\r\n",
      "           bq load ds.small gs://mybucket/small.csv field1,field2,field3\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           destination_table: Destination table name.\r\n",
      "           source: Name of local file to import, or a comma-separated list of\r\n",
      "           URI paths to data to import.\r\n",
      "           schema: Either a text schema or JSON file, as above.\r\n",
      "\r\n",
      "ls         List the objects contained in the named collection.\r\n",
      "\r\n",
      "           List the objects in the named project or dataset. A trailing : or .\r\n",
      "           can be used to signify a project or dataset.\r\n",
      "           * With -j, show the jobs in the named project.\r\n",
      "           * With -p, show all projects.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq ls\r\n",
      "           bq ls -j proj\r\n",
      "           bq ls -p -n 1000\r\n",
      "           bq ls mydataset\r\n",
      "           bq ls -a\r\n",
      "           bq ls --filter labels.color:red\r\n",
      "           bq ls --filter 'labels.color:red labels.size:*'\r\n",
      "           bq ls --transfer_config --transfer_location='us'\r\n",
      "           --filter='dataSourceIds:play,adwords'\r\n",
      "           bq ls --transfer_run --filter='states:SUCCESSED,PENDING'\r\n",
      "           --run_attempt='LATEST' projects/p/locations/l/transferConfigs/c\r\n",
      "           bq ls --transfer_log --message_type='messageTypes:INFO,ERROR'\r\n",
      "           projects/p/locations/l/transferConfigs/c/runs/r\r\n",
      "\r\n",
      "mk         Create a dataset, table, view, or transfer configuration with this\r\n",
      "           name.\r\n",
      "\r\n",
      "           See 'bq help load' for more information on specifying the schema.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq mk new_dataset\r\n",
      "           bq mk new_dataset.new_table\r\n",
      "           bq --dataset_id=new_dataset mk table\r\n",
      "           bq mk -t new_dataset.newtable name:integer,value:string\r\n",
      "           bq mk --view='select 1 as num' new_dataset.newview\r\n",
      "           (--view_udf_resource=path/to/file.js)\r\n",
      "           bq mk -d --data_location=EU new_dataset\r\n",
      "           bq mk --transfer_config --target_dataset=dataset --display_name=name\r\n",
      "           -p='{\"param\":\"value\"}' --data_source=source\r\n",
      "           bq mk --transfer_run --start_time={start_time} --end_time={end_time}\r\n",
      "           projects/p/locations/l/transferConfigs/c\r\n",
      "\r\n",
      "mkdef      Emits a definition in JSON for a GCS backed table.\r\n",
      "\r\n",
      "           The output of this command can be redirected to a file and used for\r\n",
      "           the external_table_definition flag with the \"bq query\" and \"bq mk\"\r\n",
      "           commands. It produces a definition with the most commonly used values\r\n",
      "           for options. You can modify the output to override option values.\r\n",
      "\r\n",
      "           Usage:\r\n",
      "           mkdef <source_uris> [<schema>]\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq mkdef 'gs://bucket/file.csv' field1:integer,field2:string\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           source_uris: a comma-separated list of uris.\r\n",
      "           schema: The <schema> argument should be either the name of a JSON\r\n",
      "           file or\r\n",
      "           a text schema.\r\n",
      "\r\n",
      "           In the case that the schema is provided in text form, it should be a\r\n",
      "           comma-separated list of entries of the form name[:type], where type\r\n",
      "           will\r\n",
      "           default to string if not specified.\r\n",
      "\r\n",
      "           In the case that <schema> is a filename, it should contain a\r\n",
      "           single array object, each entry of which should be an object with\r\n",
      "           properties 'name', 'type', and (optionally) 'mode'. See the online\r\n",
      "           documentation for more detail:\r\n",
      "           https://developers.google.com/bigquery/preparing-data-for-bigquery\r\n",
      "\r\n",
      "           Note: the case of a single-entry schema with no type specified is\r\n",
      "           ambiguous; one can use name:string to force interpretation as a\r\n",
      "           text schema.\r\n",
      "\r\n",
      "partition  Copies source tables into partitioned tables.\r\n",
      "\r\n",
      "           Usage: bq partition <source_table_prefix>\r\n",
      "           <destination_partitioned_table>\r\n",
      "\r\n",
      "           Copies tables of the format <source_table_prefix><YYYYmmdd> to a\r\n",
      "           destination partitioned table, with the date suffix of the source\r\n",
      "           tables becoming the partition date of the destination table\r\n",
      "           partitions.\r\n",
      "\r\n",
      "           If the destination table does not exist, one will be created with a\r\n",
      "           schema and that matches the last table that matches the supplied\r\n",
      "           prefix.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq partition dataset1.sharded_ dataset2.partitioned_table\r\n",
      "\r\n",
      "query      Execute a query.\r\n",
      "\r\n",
      "           Query should be specifed on command line, or passed on stdin.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq query 'select count(*) from publicdata:samples.shakespeare'\r\n",
      "           echo 'select count(*) from publicdata:samples.shakespeare' | bq query\r\n",
      "\r\n",
      "           Usage:\r\n",
      "           query [<sql_query>]\r\n",
      "\r\n",
      "rm         Delete the dataset, table, or transfer config described by\r\n",
      "           identifier.\r\n",
      "\r\n",
      "           Always requires an identifier, unlike the show and ls commands. By\r\n",
      "           default, also requires confirmation before deleting. Supports the -d\r\n",
      "           -t flags to signify that the identifier is a dataset or table.\r\n",
      "           * With -f, don't ask for confirmation before deleting.\r\n",
      "           * With -r, remove all tables in the named dataset.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq rm ds.table\r\n",
      "           bq rm -r -f old_dataset\r\n",
      "           bq rm --transfer_config=projects/p/locations/l/transferConfigs/c\r\n",
      "\r\n",
      "shell      Start an interactive bq session.\r\n",
      "\r\n",
      "show       Show all information about an object.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq show -j <job_id>\r\n",
      "           bq show dataset\r\n",
      "           bq show [--schema] dataset.table\r\n",
      "           bq show [--view] dataset.view\r\n",
      "           bq show --transfer_config projects/p/locations/l/transferConfigs/c\r\n",
      "           bq show --transfer_run\r\n",
      "           projects/p/locations/l/transferConfigs/c/runs/r\r\n",
      "           bq show --encryption_service_account\r\n",
      "\r\n",
      "update     Updates a dataset, table, view or transfer configuration with this\r\n",
      "           name.\r\n",
      "\r\n",
      "           See 'bq help load' for more information on specifying the schema.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq update --description \"Dataset description\" existing_dataset\r\n",
      "           bq update --description \"My table\" existing_dataset.existing_table\r\n",
      "           bq update -t existing_dataset.existing_table\r\n",
      "           name:integer,value:string\r\n",
      "           bq update --destination_kms_key\r\n",
      "           projects/p/locations/l/keyRings/r/cryptoKeys/k\r\n",
      "           existing_dataset.existing_table\r\n",
      "           bq update --view='select 1 as num' existing_dataset.existing_view\r\n",
      "           (--view_udf_resource=path/to/file.js)\r\n",
      "           bq update --transfer_config --display_name=name\r\n",
      "           -p='{\"param\":\"value\"}'\r\n",
      "           projects/p/locations/l/transferConfigs/c\r\n",
      "           bq update --transfer_config --target_dataset=dataset\r\n",
      "           --refresh_window_days=5 --update_credentials\r\n",
      "           projects/p/locations/l/transferConfigs/c\r\n",
      "\r\n",
      "version    Return the version of bq.\r\n",
      "\r\n",
      "wait       Wait some number of seconds for a job to finish.\r\n",
      "\r\n",
      "           Poll job_id until either (1) the job is DONE or (2) the specified\r\n",
      "           number of seconds have elapsed. Waits forever if unspecified. If no\r\n",
      "           job_id is specified, and there is only one running job, we poll that\r\n",
      "           job.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq wait # Waits forever for the currently running job.\r\n",
      "           bq wait job_id # Waits forever\r\n",
      "           bq wait job_id 100 # Waits 100 seconds\r\n",
      "           bq wait job_id 0 # Polls if a job is done, then returns immediately.\r\n",
      "           # These may exit with a non-zero status code to indicate \"failure\":\r\n",
      "           bq wait --fail_on_error job_id # Succeeds if job succeeds.\r\n",
      "           bq wait --fail_on_error job_id 100 # Succeeds if job succeeds in 100\r\n",
      "           sec.\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           job_id: Job ID to wait on.\r\n",
      "           secs: Number of seconds to wait (must be >= 0).\r\n",
      "\r\n",
      "\r\n",
      "Run 'bq --help' to get help for global flags.\r\n",
      "Run 'bq help <command>' to get help for <command>.\r\n"
     ]
    }
   ],
   "source": [
    "!bq help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataset\n",
    "\n",
    "A dataset is contained within a specific [project](https://cloud.google.com/bigquery/docs/projects). Datasets are top-level containers that are used to organize and control access to your [tables](https://cloud.google.com/bigquery/docs/tables) and [views](https://cloud.google.com/bigquery/docs/views). A table or view must belong to a dataset, so you need to create at least one dataset before [loading data into BigQuery](https://cloud.google.com/bigquery/loading-data-into-bigquery).\n",
    "\n",
    "The example below creates a new dataset in the US named \"your_new_dataset\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'ajhamilton-scratch:your_dataset_id' successfully created.\r\n"
     ]
    }
   ],
   "source": [
    "!bq --location=US mk --dataset \"your_dataset_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from a local file to a table\n",
    "\n",
    "The example below demonstrates how to load a local CSV file into a new or existing table. See [SourceFormat](https://googleapis.github.io/google-cloud-python/latest/bigquery/generated/google.cloud.bigquery.job.SourceFormat.html#google.cloud.bigquery.job.SourceFormat) in the Python client library documentation for a list of available source formats. For more information, see [Loading Data into BigQuery from a Local Data Source](https://cloud.google.com/bigquery/docs/loading-data-local) in the BigQuery documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python script for interacting with BigQuery.\r\n",
      "\r\n",
      "\r\n",
      "USAGE: bq [--global_flags] <command> [--command_flags] [args]\r\n",
      "\r\n",
      "\r\n",
      "load       Perform a load operation of source into destination_table.\r\n",
      "\r\n",
      "           Usage:\r\n",
      "           load <destination_table> <source> [<schema>]\r\n",
      "\r\n",
      "           The <destination_table> is the fully-qualified table name of table to\r\n",
      "           create, or append to if the table already exists.\r\n",
      "\r\n",
      "           The <source> argument can be a path to a single local file, or a\r\n",
      "           comma-separated list of URIs.\r\n",
      "\r\n",
      "           The <schema> argument should be either the name of a JSON file or a\r\n",
      "           text schema. This schema should be omitted if the table already has\r\n",
      "           one.\r\n",
      "\r\n",
      "           In the case that the schema is provided in text form, it should be a\r\n",
      "           comma-separated list of entries of the form name[:type], where type\r\n",
      "           will default to string if not specified.\r\n",
      "\r\n",
      "           In the case that <schema> is a filename, it should contain a single\r\n",
      "           array object, each entry of which should be an object with properties\r\n",
      "           'name', 'type', and (optionally) 'mode'. See the online documentation\r\n",
      "           for more detail:\r\n",
      "           https://developers.google.com/bigquery/preparing-data-for-bigquery\r\n",
      "\r\n",
      "           Note: the case of a single-entry schema with no type specified is\r\n",
      "           ambiguous; one can use name:string to force interpretation as a\r\n",
      "           text schema.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq load ds.new_tbl ./info.csv ./info_schema.json\r\n",
      "           bq load ds.new_tbl gs://mybucket/info.csv ./info_schema.json\r\n",
      "           bq load ds.small gs://mybucket/small.csv name:integer,value:string\r\n",
      "           bq load ds.small gs://mybucket/small.csv field1,field2,field3\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           destination_table: Destination table name.\r\n",
      "           source: Name of local file to import, or a comma-separated list of\r\n",
      "           URI paths to data to import.\r\n",
      "           schema: Either a text schema or JSON file, as above.\r\n",
      "\r\n",
      "           Flags for load:\r\n",
      "\r\n",
      "/Users/ajhamilton/google-cloud-sdk/platform/bq/bq.py:\r\n",
      "  --[no]allow_jagged_rows: Whether to allow missing trailing optional columns in\r\n",
      "    CSV import data.\r\n",
      "  --[no]allow_quoted_newlines: Whether to allow quoted newlines in CSV import\r\n",
      "    data.\r\n",
      "  --[no]autodetect: Enable auto detection of schema and options for formats that\r\n",
      "    are not self describing like CSV and JSON.\r\n",
      "  --clustering_fields: Comma separated field names. Can only be specified with\r\n",
      "    time based partitioning. Data will be first partitioned and subsequently\r\n",
      "    \"clustered on these fields.\r\n",
      "  --destination_kms_key: Cloud KMS key for encryption of the destination table\r\n",
      "    data.\r\n",
      "  -E,--encoding: <UTF-8|ISO-8859-1>: The character encoding used by the input\r\n",
      "    file. Options include:\r\n",
      "    ISO-8859-1 (also known as Latin-1)\r\n",
      "    UTF-8\r\n",
      "  -F,--field_delimiter: The character that indicates the boundary between\r\n",
      "    columns in the input file. \"\\t\" and \"tab\" are accepted names for tab.\r\n",
      "  --[no]ignore_unknown_values: Whether to allow and ignore extra, unrecognized\r\n",
      "    values in CSV or JSON import data.\r\n",
      "  --max_bad_records: Maximum number of bad records allowed before the entire job\r\n",
      "    fails.\r\n",
      "    (default: '0')\r\n",
      "    (an integer)\r\n",
      "  --null_marker: An optional custom string that will represent a NULL valuein\r\n",
      "    CSV import data.\r\n",
      "  --projection_fields: If sourceFormat is set to \"DATASTORE_BACKUP\", indicates\r\n",
      "    which entity properties to load into BigQuery from a Cloud Datastore backup.\r\n",
      "    Property names are case sensitive and must refer to top-level properties.\r\n",
      "    (default: '')\r\n",
      "    (a comma separated list)\r\n",
      "  --quote: Quote character to use to enclose records. Default is \". To indicate\r\n",
      "    no quote character at all, use an empty string.\r\n",
      "  --[no]replace: If true erase existing contents before loading new data.\r\n",
      "    (default: 'false')\r\n",
      "  --[no]require_partition_filter: Whether to require partition filter for\r\n",
      "    queries over this table. Only apply to partitioned table.\r\n",
      "  --schema: Either a filename or a comma-separated list of fields in the form\r\n",
      "    name[:type].\r\n",
      "  --schema_update_option: Can be specified when append to a table, or replace a\r\n",
      "    table partition. When specified, the schema of the destination table will be\r\n",
      "    updated with the schema of the new data. One or more of the following\r\n",
      "    options can be specified:\r\n",
      "    ALLOW_FIELD_ADDITION: allow new fields to be added\r\n",
      "    ALLOW_FIELD_RELAXATION: allow relaxing required fields to nullable;\r\n",
      "    repeat this option to specify a list of values\r\n",
      "  --skip_leading_rows: The number of rows at the beginning of the source file to\r\n",
      "    skip.\r\n",
      "    (an integer)\r\n",
      "  --source_format:\r\n",
      "    <CSV|NEWLINE_DELIMITED_JSON|DATASTORE_BACKUP|AVRO|PARQUET|ORC>: Format of\r\n",
      "    source data. Options include:\r\n",
      "    CSV\r\n",
      "    NEWLINE_DELIMITED_JSON\r\n",
      "    DATASTORE_BACKUP\r\n",
      "    AVRO\r\n",
      "    PARQUET\r\n",
      "    ORC (experimental)\r\n",
      "  --time_partitioning_expiration: Enables time based partitioning on the table\r\n",
      "    and sets the number of seconds for which to keep the storage for the\r\n",
      "    partitions in the table. The storage in a partition will have an expiration\r\n",
      "    time of its partition time plus this value. A negative number means no\r\n",
      "    expiration.\r\n",
      "    (an integer)\r\n",
      "  --time_partitioning_field: Enables time based partitioning on the table and\r\n",
      "    the table will be partitioned based on the value of this field. If time\r\n",
      "    based partitioning is enabled without this value, the table will be\r\n",
      "    partitioned based on the loading time.\r\n",
      "  --time_partitioning_type: Enables time based partitioning on the table and set\r\n",
      "    the type. The only type accepted is DAY, which will generate one partition\r\n",
      "    per day.\r\n",
      "\r\n",
      "gflags:\r\n",
      "  --flagfile: Insert flag definitions from the given file into the command line.\r\n",
      "    (default: '')\r\n",
      "  --undefok: comma-separated list of flag names that it is okay to specify on\r\n",
      "    the command line even if the program does not define a flag with that name.\r\n",
      "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\r\n",
      "    format.\r\n",
      "    (default: '')\r\n",
      "\r\n",
      "\r\n",
      "Global flags:\r\n",
      "\r\n",
      "bq_auth_flags:\r\n",
      "  --application_default_credential_file: Only for the gcloud wrapper use.\r\n",
      "    (default: '')\r\n",
      "  --credential_file: Only for the gcloud wrapper use.\r\n",
      "    (default: '/Users/ajhamilton/.bigquery.v2.token')\r\n",
      "  --service_account: Only for the gcloud wrapper use.\r\n",
      "    (default: '')\r\n",
      "  --service_account_credential_file: Only for the gcloud wrapper use.\r\n",
      "  --service_account_private_key_file: Only for the gcloud wrapper use.\r\n",
      "    (default: '')\r\n",
      "  --service_account_private_key_password: Only for the gcloud wrapper use.\r\n",
      "    (default: 'notasecret')\r\n",
      "  --[no]use_gce_service_account: Only for the gcloud wrapper use.\r\n",
      "    (default: 'false')\r\n",
      "\r\n",
      "bq_flags:\r\n",
      "  --api: API endpoint to talk to.\r\n",
      "    (default: 'https://www.googleapis.com')\r\n",
      "  --api_version: API version to use.\r\n",
      "    (default: 'v2')\r\n",
      "  --apilog: Log all API requests and responses to the file specified by this\r\n",
      "    flag. Also accepts \"stdout\" and \"stderr\". Specifying the empty string will\r\n",
      "    direct to stdout.\r\n",
      "  --bigqueryrc: Path to configuration file. The configuration file specifies new\r\n",
      "    defaults for any flags, and can be overrridden by specifying the flag on the\r\n",
      "    command line. If the --bigqueryrc flag is not specified, the BIGQUERYRC\r\n",
      "    environment variable is used. If that is not specified, the path\r\n",
      "    \"~/.bigqueryrc\" is used.\r\n",
      "    (default: '/Users/ajhamilton/.bigqueryrc')\r\n",
      "  --ca_certificates_file: Location of CA certificates file.\r\n",
      "    (default: '')\r\n",
      "  --dataset_id: Default dataset reference to use for requests (Ignored when not\r\n",
      "    applicable.). Can be set as \"project:dataset\" or \"dataset\". If project is\r\n",
      "    missing, the value of the project_id flag will be used.\r\n",
      "    (default: '')\r\n",
      "  --[no]debug_mode: Show tracebacks on Python exceptions.\r\n",
      "    (default: 'false')\r\n",
      "  --[no]disable_ssl_validation: Disables HTTPS certificates validation. This is\r\n",
      "    off by default.\r\n",
      "    (default: 'false')\r\n",
      "  --discovery_file: Filename for JSON document to read for discovery.\r\n",
      "    (default: '')\r\n",
      "  --[no]enable_gdrive: When set to true, requests new OAuth token with GDrive\r\n",
      "    scope. When set to false, requests new OAuth token without GDrive scope.\r\n",
      "  --[no]fingerprint_job_id: Whether to use a job id that is derived from a\r\n",
      "    fingerprint of the job configuration. This will prevent the same job from\r\n",
      "    running multiple times accidentally.\r\n",
      "    (default: 'false')\r\n",
      "  --format: <none|json|prettyjson|csv|sparse|pretty>: Format for command output.\r\n",
      "    Options include:\r\n",
      "    pretty: formatted table output\r\n",
      "    sparse: simpler table output\r\n",
      "    prettyjson: easy-to-read JSON format\r\n",
      "    json: maximally compact JSON\r\n",
      "    csv: csv format with header\r\n",
      "    The first three are intended to be human-readable, and the latter three are\r\n",
      "    for passing to another program. If no format is selected, one will be chosen\r\n",
      "    based on the command run.\r\n",
      "  --[no]headless: Whether this bq session is running without user interaction.\r\n",
      "    This affects behavior that expects user interaction, like whether debug_mode\r\n",
      "    will break into the debugger and lowers the frequency of informational\r\n",
      "    printing.\r\n",
      "    (default: 'false')\r\n",
      "  --httplib2_debuglevel: Instruct httplib2 to print debugging messages by\r\n",
      "    setting debuglevel to the given value.\r\n",
      "  --job_id: A unique job_id to use for the request. If not specified, this\r\n",
      "    client will generate a job_id. Applies only to commands that launch jobs,\r\n",
      "    such as cp, extract, load, and query.\r\n",
      "  --job_property: Additional key-value pairs to include in the properties field\r\n",
      "    of the job configuration;\r\n",
      "    repeat this option to specify a list of values\r\n",
      "  --location: Default geographic location to use when creating datasets or\r\n",
      "    determining where jobs should run (Ignored when not applicable.)\r\n",
      "  --max_rows_per_request: Specifies the max number of rows to return per read.\r\n",
      "    (an integer)\r\n",
      "  --project_id: Default project to use for requests.\r\n",
      "    (default: '')\r\n",
      "  --proxy_address: The name or IP address of the proxy host to use for\r\n",
      "    connecting to GCP.\r\n",
      "    (default: '')\r\n",
      "  --proxy_password: The password to use when authenticating with proxy host.\r\n",
      "    (default: '')\r\n",
      "  --proxy_port: The port number to use to connect to the proxy host.\r\n",
      "    (default: '')\r\n",
      "  --proxy_username: The user name to use when authenticating with proxy host.\r\n",
      "    (default: '')\r\n",
      "  -q,--[no]quiet: If True, ignore status updates while jobs are running.\r\n",
      "    (default: 'false')\r\n",
      "  -sync,--[no]synchronous_mode: If True, wait for command completion before\r\n",
      "    returning, and use the job completion status for error codes. If False,\r\n",
      "    simply create the job, and use the success of job creation as the error\r\n",
      "    code.\r\n",
      "    (default: 'true')\r\n",
      "  --trace: A tracing token of the form \"token:<token>\" to include in api\r\n",
      "    requests.\r\n",
      "\r\n",
      "google.apputils.app:\r\n",
      "  -?,--[no]help: show this help\r\n",
      "  --[no]helpshort: show usage only for this module\r\n",
      "  --[no]helpxml: like --help, but generates XML output\r\n",
      "  --[no]run_with_pdb: Set to true for PDB debug mode\r\n",
      "    (default: 'false')\r\n",
      "  --[no]run_with_profiling: Set to true for profiling the script. Execution will\r\n",
      "    be slower, and the output format might change over time.\r\n",
      "    (default: 'false')\r\n",
      "  --[no]show_build_data: show build data and exit\r\n",
      "  --[no]use_cprofile_for_profiling: Use cProfile instead of the profile module\r\n",
      "    for profiling. This has no effect unless --run_with_profiling is set.\r\n",
      "    (default: 'true')\r\n",
      "\r\n",
      "gflags:\r\n",
      "  --flagfile: Insert flag definitions from the given file into the command line.\r\n",
      "    (default: '')\r\n",
      "  --undefok: comma-separated list of flag names that it is okay to specify on\r\n",
      "    the command line even if the program does not define a flag with that name.\r\n",
      "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\r\n",
      "    format.\r\n",
      "    (default: '')\r\n",
      "\r\n",
      "Run 'bq help' to see the list of available commands.\r\n"
     ]
    }
   ],
   "source": [
    "!bq load --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Upload complete.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Waiting on bqjob_r3fea50bb473556c6_000001687c0441ee_1 ... (0s) Current status: RUNNING"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3fea50bb473556c6_000001687c0441ee_1 ... (1s) Current status: RUNNING"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3fea50bb473556c6_000001687c0441ee_1 ... (2s) Current status: RUNNING"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3fea50bb473556c6_000001687c0441ee_1 ... (2s) Current status: DONE   \r\n"
     ]
    }
   ],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 --source_format=CSV your_dataset_id.us_states_local_file 'resources/us-states.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Google Cloud Storage to a table\n",
    "\n",
    "The example below demonstrates how to load a local CSV file into a new or existing table. See [SourceFormat](https://googleapis.github.io/google-cloud-python/latest/bigquery/generated/google.cloud.bigquery.job.SourceFormat.html#google.cloud.bigquery.job.SourceFormat) in the Python client library documentation for a list of available source formats. For more information, see [Introduction to Loading Data from Cloud Storage](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage) in the BigQuery documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Waiting on bqjob_r3c677e7ba7eb1551_000001687c045ffb_1 ... (0s) Current status: RUNNING"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3c677e7ba7eb1551_000001687c045ffb_1 ... (1s) Current status: RUNNING"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3c677e7ba7eb1551_000001687c045ffb_1 ... (1s) Current status: DONE   \r\n"
     ]
    }
   ],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 --source_format=CSV your_dataset_id.us_states_gcs 'gs://cloud-samples-data/bigquery/us-states/us-states.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a query\n",
    "\n",
    "The BigQuery command-line tool has a `query` command for running queries, but it is recommended to use the [Magic command](./BigQuery%20Query%20Magic.ipynb) for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "The following code deletes the dataset created for this tutorial, including all tables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq rm -r -f --dataset your_dataset_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local google-cloud-bigquery development",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
